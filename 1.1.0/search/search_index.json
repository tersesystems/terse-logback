{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Terse Logback \u00b6 Terse Logback is a collection of Logback modules that extend Logback functionality. I've written about the reasoning and internal architecture in a series of blog posts. The full list is available on https://tersesystems.com . Installation \u00b6 You can find modules using https://mvnrepository.com/artifact/com.tersesystems.logback . For example, the typesafe config module can be found under https://mvnrepository.com/artifact/com.tersesystems.logback/logback-typesafe-config/1.0.3 and selecting Maven will let you paste it: <dependency> <groupId> com.tersesystems.logback </groupId> <artifactId> logback-typesafe-config </artifactId> <version> $VERSION$ </version> </dependency> Showcase \u00b6 If you want to see a running application, there is a showcase web application that run out of the box that demonstrates some of the more advanced features, and shows you can integrate terse-logback with Sentry and Honeycomb . Modules \u00b6 Audio : Play audio when you log by attaching markers to your logging statements. Budgeting / Rate Limiting : Limit the amount of debugging or tracing statements in a time period. Censors : Censor sensitive information in logging statements. Composite : Presents a single appender that composes several appenders. Compression : Write to a compressed zstandard file. Correlation Id : Adds markers and filters for correlation id. Exception Mapping : Show the important details of an exception, including the root cause in a summary format. Instrumentation : Decorates any (including JVM) class with enter and exit logging statements at runtime. JDBC : Use Postgres JSON to write structured logging to a single table. JUL to SLF4J Bridge : Configure java.util.logging to write to SLF4J with no manual coding . Relative Nanos : Composes a logging event to contain relative nanoseconds based off System.nanoTime . Select Appender : Appender that selects an appender from a list based on key. Tracing : Sends logging events and traces to Honeycomb Event API . Typesafe Config : Configure Logback properties using HOCON . Turbo Markers : Turbo Filters that depend on arbitrary deciders that can log at debug level for sessions. Unique ID Appender : Composes logging event to contain a unique id across multiple appenders.","title":"Home"},{"location":"#terse-logback","text":"Terse Logback is a collection of Logback modules that extend Logback functionality. I've written about the reasoning and internal architecture in a series of blog posts. The full list is available on https://tersesystems.com .","title":"Terse Logback"},{"location":"#installation","text":"You can find modules using https://mvnrepository.com/artifact/com.tersesystems.logback . For example, the typesafe config module can be found under https://mvnrepository.com/artifact/com.tersesystems.logback/logback-typesafe-config/1.0.3 and selecting Maven will let you paste it: <dependency> <groupId> com.tersesystems.logback </groupId> <artifactId> logback-typesafe-config </artifactId> <version> $VERSION$ </version> </dependency>","title":"Installation"},{"location":"#showcase","text":"If you want to see a running application, there is a showcase web application that run out of the box that demonstrates some of the more advanced features, and shows you can integrate terse-logback with Sentry and Honeycomb .","title":"Showcase"},{"location":"#modules","text":"Audio : Play audio when you log by attaching markers to your logging statements. Budgeting / Rate Limiting : Limit the amount of debugging or tracing statements in a time period. Censors : Censor sensitive information in logging statements. Composite : Presents a single appender that composes several appenders. Compression : Write to a compressed zstandard file. Correlation Id : Adds markers and filters for correlation id. Exception Mapping : Show the important details of an exception, including the root cause in a summary format. Instrumentation : Decorates any (including JVM) class with enter and exit logging statements at runtime. JDBC : Use Postgres JSON to write structured logging to a single table. JUL to SLF4J Bridge : Configure java.util.logging to write to SLF4J with no manual coding . Relative Nanos : Composes a logging event to contain relative nanoseconds based off System.nanoTime . Select Appender : Appender that selects an appender from a list based on key. Tracing : Sends logging events and traces to Honeycomb Event API . Typesafe Config : Configure Logback properties using HOCON . Turbo Markers : Turbo Filters that depend on arbitrary deciders that can log at debug level for sessions. Unique ID Appender : Composes logging event to contain a unique id across multiple appenders.","title":"Modules"},{"location":"guide/audio/","text":"Audio \u00b6 The audio appender uses a system beep configured through SystemPlayer to notify on warnings and errors, and limits excessive beeps with a budget evaluator. The XML is as follows: <included> <appender name= \"AUDIO-WARN\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> WARN </level> <onMatch> NEUTRAL </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO-ERROR\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> ERROR </level> <onMatch> ACCEPT </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budgetRule name= \"WARN\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> <budgetRule name= \"ERROR\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <appender-ref ref= \"AUDIO-WARN\" /> <appender-ref ref= \"AUDIO-ERROR\" /> </appender> </included> See Application Logging in Java: Appenders for more details.","title":"Audio"},{"location":"guide/audio/#audio","text":"The audio appender uses a system beep configured through SystemPlayer to notify on warnings and errors, and limits excessive beeps with a budget evaluator. The XML is as follows: <included> <appender name= \"AUDIO-WARN\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> WARN </level> <onMatch> NEUTRAL </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO-ERROR\" class= \"com.tersesystems.logback.audio.AudioAppender\" > <filter class= \"ch.qos.logback.classic.filter.LevelFilter\" > <level> ERROR </level> <onMatch> ACCEPT </onMatch> <onMismatch> DENY </onMismatch> </filter> <player class= \"com.tersesystems.logback.audio.SystemPlayer\" /> </appender> <appender name= \"AUDIO\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budgetRule name= \"WARN\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> <budgetRule name= \"ERROR\" threshold= \"1\" interval= \"5\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <appender-ref ref= \"AUDIO-WARN\" /> <appender-ref ref= \"AUDIO-ERROR\" /> </appender> </included> See Application Logging in Java: Appenders for more details.","title":"Audio"},{"location":"guide/budget/","text":"Budget Aware Logging \u00b6 There are instances where loggers may be overly chatty, and will log more than necessary. Rather than hunt down all the individual loggers and whitelist or blacklist the lot of them, you may want to assign a budget that will budget INFO messages to 5 statements a second. This is easy to do with the logback-budget module, which uses an internal circuit breaker to regulate the flow of messages. <configuration> <newRule pattern= \"*/budget-rule\" actionClass= \"com.tersesystems.logback.budget.BudgetRuleAction\" /> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budget-rule name= \"INFO\" threshold= \"5\" interval= \"1\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> See Application Logging in Java: Filters for more details.","title":"Budgeting / Rate Limiting"},{"location":"guide/budget/#budget-aware-logging","text":"There are instances where loggers may be overly chatty, and will log more than necessary. Rather than hunt down all the individual loggers and whitelist or blacklist the lot of them, you may want to assign a budget that will budget INFO messages to 5 statements a second. This is easy to do with the logback-budget module, which uses an internal circuit breaker to regulate the flow of messages. <configuration> <newRule pattern= \"*/budget-rule\" actionClass= \"com.tersesystems.logback.budget.BudgetRuleAction\" /> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <filter class= \"ch.qos.logback.core.filter.EvaluatorFilter\" > <evaluator class= \"com.tersesystems.logback.budget.BudgetEvaluator\" > <budget-rule name= \"INFO\" threshold= \"5\" interval= \"1\" timeUnit= \"seconds\" /> </evaluator> <OnMismatch> DENY </OnMismatch> <OnMatch> NEUTRAL </OnMatch> </filter> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> See Application Logging in Java: Filters for more details.","title":"Budget Aware Logging"},{"location":"guide/censor/","text":"Censors \u00b6 There may be sensitive information that you don't want to show up in the logs. You can get around this by passing your information through a censor. This is a custom bit of code written for Logback, but it's not too complex. There are two rules and a converter that are used in Logback to define and reference censors: CensorAction , CensorRefAction and the censor converter. <configuration> <newRule pattern= \"*/censor\" actionClass= \"com.tersesystems.logback.censor.CensorAction\" /> <newRule pattern= \"*/censor-ref\" actionClass= \"com.tersesystems.logback.censor.CensorRefAction\" /> <conversionRule conversionWord= \"censor\" converterClass= \"com.tersesystems.logback.censor.CensorConverter\" /> <!-- ... --> </configuration> The CensorAction defines a censor that can be referred to by the CensorRef action and the censor conversionWord, using the censor name. The default implementation is the regex censor, which will look for a regular expression and replace it with the replacement text defined: <configuration> <censor name= \"censor-name1\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR1] </replacementText> <regex> hunter1 </regex> </censor> <censor name= \"censor-name2\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR2] </replacementText> <regex> hunter2 </regex> </censor> </configuration> Once you have the censors defined, you can use the censor word by specifying the target as defined in the pattern encoder format , and adding the name as the option list using curly braces, i.e. %censor(%msg){censor-name1} . If you don't define the censor, then the first available censor will be picked. <configuration> <appender name= \"TEST1\" class= \"ch.qos.logback.core.FileAppender\" > <file> file1.log </file> <encoder> <pattern> %censor(%msg){censor-name1}%n </pattern> </encoder> </appender> <appender name= \"TEST2\" class= \"ch.qos.logback.core.FileAppender\" > <file> file2.log </file> <encoder> <pattern> %censor(%msg){censor-name2}%n </pattern> </encoder> </appender> </configuration> If you are working with a componentized framework, you'll want to use the censor-ref action instead. Here's an example using logstash-logback-encoder. <configuration> <appender name= \"TEST3\" class= \"ch.qos.logback.core.FileAppender\" > <file> file3.log </file> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringJsonGeneratorDecorator\" > <censor-ref ref= \"first-censor\" /> <censor-ref ref= \"second-censor\" /> </jsonGeneratorDecorator> </encoder> </appender> </configuration> In this case, CensoringJsonGeneratorDecorator implements the CensorAttachable interface and so will run message text through the censor if it exists. See Application Logging in Java: Converters for more details.","title":"Censors"},{"location":"guide/censor/#censors","text":"There may be sensitive information that you don't want to show up in the logs. You can get around this by passing your information through a censor. This is a custom bit of code written for Logback, but it's not too complex. There are two rules and a converter that are used in Logback to define and reference censors: CensorAction , CensorRefAction and the censor converter. <configuration> <newRule pattern= \"*/censor\" actionClass= \"com.tersesystems.logback.censor.CensorAction\" /> <newRule pattern= \"*/censor-ref\" actionClass= \"com.tersesystems.logback.censor.CensorRefAction\" /> <conversionRule conversionWord= \"censor\" converterClass= \"com.tersesystems.logback.censor.CensorConverter\" /> <!-- ... --> </configuration> The CensorAction defines a censor that can be referred to by the CensorRef action and the censor conversionWord, using the censor name. The default implementation is the regex censor, which will look for a regular expression and replace it with the replacement text defined: <configuration> <censor name= \"censor-name1\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR1] </replacementText> <regex> hunter1 </regex> </censor> <censor name= \"censor-name2\" class= \"com.tersesystems.logback.censor.RegexCensor\" > <replacementText> [CENSORED BY CENSOR2] </replacementText> <regex> hunter2 </regex> </censor> </configuration> Once you have the censors defined, you can use the censor word by specifying the target as defined in the pattern encoder format , and adding the name as the option list using curly braces, i.e. %censor(%msg){censor-name1} . If you don't define the censor, then the first available censor will be picked. <configuration> <appender name= \"TEST1\" class= \"ch.qos.logback.core.FileAppender\" > <file> file1.log </file> <encoder> <pattern> %censor(%msg){censor-name1}%n </pattern> </encoder> </appender> <appender name= \"TEST2\" class= \"ch.qos.logback.core.FileAppender\" > <file> file2.log </file> <encoder> <pattern> %censor(%msg){censor-name2}%n </pattern> </encoder> </appender> </configuration> If you are working with a componentized framework, you'll want to use the censor-ref action instead. Here's an example using logstash-logback-encoder. <configuration> <appender name= \"TEST3\" class= \"ch.qos.logback.core.FileAppender\" > <file> file3.log </file> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > <jsonGeneratorDecorator class= \"com.tersesystems.logback.censor.CensoringJsonGeneratorDecorator\" > <censor-ref ref= \"first-censor\" /> <censor-ref ref= \"second-censor\" /> </jsonGeneratorDecorator> </encoder> </appender> </configuration> In this case, CensoringJsonGeneratorDecorator implements the CensorAttachable interface and so will run message text through the censor if it exists. See Application Logging in Java: Converters for more details.","title":"Censors"},{"location":"guide/composite/","text":"Composite Appender \u00b6 The composite appender presents a single appender and appends to several appenders. It is very useful for referring to a list of appenders by a single name. <configuration debug= \"true\" > <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"FILE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"CONSOLE_AND_FILE\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"FILE\" /> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE_AND_FILE\" /> </root> </configuration> You can leverage nesting to keep your filtering logic under control. For example, you may want to have several things happen when you hit an error in your logs. Appenders will always write when they receive an event, unless they are filtered. Using nesting, you can declare the filter once, and have the child appenders \"inherit\" that filter: <!-- Filter is on the appender chain --> <appender name=\"ERROR-APPENDER\" class=\"com.tersesystems.logback.CompositeAppender\"> <filter class=\"ch.qos.logback.classic.filter.LevelFilter\"> <level>ERROR</level> <onMatch>ACCEPT</onMatch> <onMismatch>DENY</onMismatch> </filter> <appender class=\"ch.qos.logback.core.FileAppender\"> <file>error.log</file> <encoder> <pattern>%date - %message</pattern> </encoder> </appender> <appender class=\"com.tersesystems.logback.audio.AudioAppender\"> <player class=\"com.tersesystems.logback.audio.ResourcePlayer\"> <resource>/error.ogg</resource> </player> </appender> </appender> <root level=\"TRACE\"> <appender-ref ref=\"ALL-APPENDER\"/> <appender-ref ref=\"TRACE-APPENDER\"/> <appender-ref ref=\"DEBUG-APPENDER\"/> <appender-ref ref=\"INFO-APPENDER\"/> <appender-ref ref=\"WARN-APPENDER\"/> <appender-ref ref=\"ERROR-APPENDER\"/> </root> This makes your appender logic much cleaner. See Application Logging in Java: Appenders for more details.","title":"Composite"},{"location":"guide/composite/#composite-appender","text":"The composite appender presents a single appender and appends to several appenders. It is very useful for referring to a list of appenders by a single name. <configuration debug= \"true\" > <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"FILE\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <appender name= \"CONSOLE_AND_FILE\" class= \"com.tersesystems.logback.core.CompositeAppender\" > <appender-ref ref= \"CONSOLE\" /> <appender-ref ref= \"FILE\" /> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE_AND_FILE\" /> </root> </configuration> You can leverage nesting to keep your filtering logic under control. For example, you may want to have several things happen when you hit an error in your logs. Appenders will always write when they receive an event, unless they are filtered. Using nesting, you can declare the filter once, and have the child appenders \"inherit\" that filter: <!-- Filter is on the appender chain --> <appender name=\"ERROR-APPENDER\" class=\"com.tersesystems.logback.CompositeAppender\"> <filter class=\"ch.qos.logback.classic.filter.LevelFilter\"> <level>ERROR</level> <onMatch>ACCEPT</onMatch> <onMismatch>DENY</onMismatch> </filter> <appender class=\"ch.qos.logback.core.FileAppender\"> <file>error.log</file> <encoder> <pattern>%date - %message</pattern> </encoder> </appender> <appender class=\"com.tersesystems.logback.audio.AudioAppender\"> <player class=\"com.tersesystems.logback.audio.ResourcePlayer\"> <resource>/error.ogg</resource> </player> </appender> </appender> <root level=\"TRACE\"> <appender-ref ref=\"ALL-APPENDER\"/> <appender-ref ref=\"TRACE-APPENDER\"/> <appender-ref ref=\"DEBUG-APPENDER\"/> <appender-ref ref=\"INFO-APPENDER\"/> <appender-ref ref=\"WARN-APPENDER\"/> <appender-ref ref=\"ERROR-APPENDER\"/> </root> This makes your appender logic much cleaner. See Application Logging in Java: Appenders for more details.","title":"Composite Appender"},{"location":"guide/compression/","text":"Compression \u00b6 Encoders are powerful and useful. They give you access to the raw bytes, and let you manipulate them before they get to an appender. But you'll have to put them together inside an appender if you want to do byte transformation. As an example, say that we want to write out files directly in zstandard or brotli using Logback. The easiest way to do this is to provide a FileAppender with a swapped out compression encoder, while presenting a public API that looks just like a regular encoder. Here's the appender as logback.xml sees it: <appender name= \"COMPRESS_FILE\" class= \"com.tersesystems.logback.compress.CompressingFileAppender\" > <file> encoded.zst </file> <compressAlgo> zstd </compressAlgo> <bufferSize> 1024000 </bufferSize> <encoder class= \"ch.qos.logback.classic.encoder.PatternLayoutEncoder\" > <charset> UTF-8 </charset> <pattern> %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> Under the hood, CompressingFileAppender delegates to a regular file appender, but uses commons-compress and a CompressingEncoder to wrap PatternLayoutEncoder : public class CompressingFileAppender < E > extends UnsynchronizedAppenderBase < E > { // ... @Override public void start () { fileAppender = new FileAppender <> (); fileAppender . setContext ( getContext ()); fileAppender . setFile ( getFile ()); fileAppender . setImmediateFlush ( false ); fileAppender . setPrudent ( isPrudent ()); fileAppender . setAppend ( isAppend ()); fileAppender . setName ( name + \"-embedded-file\" ); CompressingEncoder < E > compressedEncoder = createCompressingEncoder ( getEncoder ()); fileAppender . setEncoder ( compressedEncoder ); fileAppender . start (); super . start (); } public void stop () { fileAppender . stop (); super . stop (); } @Override protected void append ( E eventObject ) { fileAppender . doAppend ( eventObject ); } protected CompressingEncoder < E > createCompressingEncoder ( Encoder < E > e ) { int bufferSize = getBufferSize (); String compressAlgo = getCompressAlgo (); CompressorStreamFactory factory = CompressorStreamFactory . getSingleton (); Set < String > names = factory . getOutputStreamCompressorNames (); if ( names . contains ( getCompressAlgo ())) { try { return new CompressingEncoder <> ( e , compressAlgo , factory , bufferSize ); } catch ( CompressorException ex ) { throw new RuntimeException ( \"Cannot create CompressingEncoder\" , ex ); } } else { throw new RuntimeException ( \"No such compression algorithm: \" + compressAlgo ); } } } From there, the encoder will shove all the input bytes into a compressed stream until there's enough data to make compression worthwhile, and then flush the compressed bytes out through a byte array output stream: public class CompressingEncoder < E > extends EncoderBase < E > { private final Accumulator accumulator ; private final Encoder < E > encoder ; public CompressingEncoder ( Encoder < E > encoder , String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . encoder = encoder ; this . accumulator = new Accumulator ( compressAlgo , factory , bufferSize ); } @Override public byte [] headerBytes () { try { return accumulator . apply ( encoder . headerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] encode ( E event ) { try { return accumulator . apply ( encoder . encode ( event )); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] footerBytes () { try { return accumulator . drain ( encoder . footerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } static class Accumulator { private final ByteArrayOutputStream byteOutputStream ; private final CompressorOutputStream stream ; private final LongAdder count = new LongAdder (); private final int bufferSize ; public Accumulator ( String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . bufferSize = bufferSize ; this . byteOutputStream = new ByteArrayOutputStream (); this . stream = factory . createCompressorOutputStream ( compressAlgo , byteOutputStream ); } boolean isFlushable () { return count . intValue () >= bufferSize ; } byte [] apply ( byte [] bytes ) throws IOException { count . add ( bytes . length ); stream . write ( bytes ); if ( isFlushable ()) { stream . flush (); byte [] output = byteOutputStream . toByteArray (); byteOutputStream . reset (); count . reset (); return output ; } else { return new byte [ 0 ] ; } } byte [] drain ( byte [] inputBytes ) throws IOException { if ( inputBytes != null ) { stream . write ( inputBytes ); } stream . close (); count . reset (); return byteOutputStream . toByteArray (); } } } This keeps both FileAppender and PatternLayoutEncoder happy, while feeding compressed bytes as the stream. Using delegation is generally much easier than trying to extend from FileAppender , because FileAppender has very definite ideas about what kind of output stream it is using, and has all the logic of file rotation and backups encorporated into it, including its own gzip compression scheme for rotated files. You can also extend this to add dictionary support for ZStandard, and that would remove the need for a buffer to provide effective compression. This does come with the downside of needing to pass the dictionary out of band though. See Application Logging in Java: Encoders for more details.","title":"Compression"},{"location":"guide/compression/#compression","text":"Encoders are powerful and useful. They give you access to the raw bytes, and let you manipulate them before they get to an appender. But you'll have to put them together inside an appender if you want to do byte transformation. As an example, say that we want to write out files directly in zstandard or brotli using Logback. The easiest way to do this is to provide a FileAppender with a swapped out compression encoder, while presenting a public API that looks just like a regular encoder. Here's the appender as logback.xml sees it: <appender name= \"COMPRESS_FILE\" class= \"com.tersesystems.logback.compress.CompressingFileAppender\" > <file> encoded.zst </file> <compressAlgo> zstd </compressAlgo> <bufferSize> 1024000 </bufferSize> <encoder class= \"ch.qos.logback.classic.encoder.PatternLayoutEncoder\" > <charset> UTF-8 </charset> <pattern> %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> Under the hood, CompressingFileAppender delegates to a regular file appender, but uses commons-compress and a CompressingEncoder to wrap PatternLayoutEncoder : public class CompressingFileAppender < E > extends UnsynchronizedAppenderBase < E > { // ... @Override public void start () { fileAppender = new FileAppender <> (); fileAppender . setContext ( getContext ()); fileAppender . setFile ( getFile ()); fileAppender . setImmediateFlush ( false ); fileAppender . setPrudent ( isPrudent ()); fileAppender . setAppend ( isAppend ()); fileAppender . setName ( name + \"-embedded-file\" ); CompressingEncoder < E > compressedEncoder = createCompressingEncoder ( getEncoder ()); fileAppender . setEncoder ( compressedEncoder ); fileAppender . start (); super . start (); } public void stop () { fileAppender . stop (); super . stop (); } @Override protected void append ( E eventObject ) { fileAppender . doAppend ( eventObject ); } protected CompressingEncoder < E > createCompressingEncoder ( Encoder < E > e ) { int bufferSize = getBufferSize (); String compressAlgo = getCompressAlgo (); CompressorStreamFactory factory = CompressorStreamFactory . getSingleton (); Set < String > names = factory . getOutputStreamCompressorNames (); if ( names . contains ( getCompressAlgo ())) { try { return new CompressingEncoder <> ( e , compressAlgo , factory , bufferSize ); } catch ( CompressorException ex ) { throw new RuntimeException ( \"Cannot create CompressingEncoder\" , ex ); } } else { throw new RuntimeException ( \"No such compression algorithm: \" + compressAlgo ); } } } From there, the encoder will shove all the input bytes into a compressed stream until there's enough data to make compression worthwhile, and then flush the compressed bytes out through a byte array output stream: public class CompressingEncoder < E > extends EncoderBase < E > { private final Accumulator accumulator ; private final Encoder < E > encoder ; public CompressingEncoder ( Encoder < E > encoder , String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . encoder = encoder ; this . accumulator = new Accumulator ( compressAlgo , factory , bufferSize ); } @Override public byte [] headerBytes () { try { return accumulator . apply ( encoder . headerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] encode ( E event ) { try { return accumulator . apply ( encoder . encode ( event )); } catch ( IOException e ) { throw new RuntimeException ( e ); } } @Override public byte [] footerBytes () { try { return accumulator . drain ( encoder . footerBytes ()); } catch ( IOException e ) { throw new RuntimeException ( e ); } } static class Accumulator { private final ByteArrayOutputStream byteOutputStream ; private final CompressorOutputStream stream ; private final LongAdder count = new LongAdder (); private final int bufferSize ; public Accumulator ( String compressAlgo , CompressorStreamFactory factory , int bufferSize ) throws CompressorException { this . bufferSize = bufferSize ; this . byteOutputStream = new ByteArrayOutputStream (); this . stream = factory . createCompressorOutputStream ( compressAlgo , byteOutputStream ); } boolean isFlushable () { return count . intValue () >= bufferSize ; } byte [] apply ( byte [] bytes ) throws IOException { count . add ( bytes . length ); stream . write ( bytes ); if ( isFlushable ()) { stream . flush (); byte [] output = byteOutputStream . toByteArray (); byteOutputStream . reset (); count . reset (); return output ; } else { return new byte [ 0 ] ; } } byte [] drain ( byte [] inputBytes ) throws IOException { if ( inputBytes != null ) { stream . write ( inputBytes ); } stream . close (); count . reset (); return byteOutputStream . toByteArray (); } } } This keeps both FileAppender and PatternLayoutEncoder happy, while feeding compressed bytes as the stream. Using delegation is generally much easier than trying to extend from FileAppender , because FileAppender has very definite ideas about what kind of output stream it is using, and has all the logic of file rotation and backups encorporated into it, including its own gzip compression scheme for rotated files. You can also extend this to add dictionary support for ZStandard, and that would remove the need for a buffer to provide effective compression. This does come with the downside of needing to pass the dictionary out of band though. See Application Logging in Java: Encoders for more details.","title":"Compression"},{"location":"guide/correlationid/","text":"Correlation ID \u00b6 The logback-correlationid module is a set of classes designed to encompass the idea of a correlation id in events. It consists of a correlation id filter, a tap filter that always logs events with a correlation id to an appender, and a correlation id marker. Correlation ID Filter \u00b6 A correlation id filter will filter for a correlation id set either as an MDC value, or as a marker created from CorrelationIdMarker . <appender name= \"LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > <filter class= \"com.tersesystems.logback.correlationid.CorrelationIdFilter\" > <mdcKey> correlationId </mdcKey> </filter> </appender> If an appender passes the filter, it will log the event. public class CorrelationIdFilterTest { public void testFilter () { // Write something that never gets logged explicitly... Logger logger = loggerFactory . getLogger ( \"com.example.Debug\" ); String correlationId = \"12345\" ; CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); // should be logged because marker logger . info ( correlationIdMarker , \"info one\" ); logger . info ( \"info two\" ); // should not be logged // Everything below this point should be logged. MDC . put ( \"correlationId\" , correlationId ); logger . info ( \"info three\" ); // should not be logged logger . info ( correlationIdMarker , \"info four\" ); } } CorrelationIdTapFilter \u00b6 The CorrelationIdTapFilter is a turbofilter that always logs to a given appender if the correlation id appears, even if the appender is not configured for logging. This functions as a wiretap . Tap Filters are very useful as a way to send data to an appender. They completely bypass any kind of logging level configured on the front end, so you can set a logger to INFO level but still have access to all TRACE events when an error occurs, through the tap filter's appenders. For example, a tap filter can automatically log everything with a correlation id at a TRACE level, without requiring filters or altering the log level as a whole. Let's run a simple HTTP client program that calls out to Google and prints a result. <configuration> <newRule pattern= \"configuration/turboFilter/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <appender name= \"TAP_LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <turboFilter class= \"com.tersesystems.logback.correlationid.CorrelationIdTapFilter\" > <mdcKey> correlationId </mdcKey> <appender-ref ref= \"TAP_LIST\" /> </turboFilter> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> CorrelationIdMarker \u00b6 A CorrelationIdMarker implements the CorrelationIdProvider interface to expose a marker which is known to contain a correlation id. CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); String sameId = correlationIdMarker . getCorrelationId (); CorrelationIdUtils \u00b6 CorrelationIdUtils contains utility methods like get which retrieve a correlation id from either a marker or MDC.","title":"Correlation Id"},{"location":"guide/correlationid/#correlation-id","text":"The logback-correlationid module is a set of classes designed to encompass the idea of a correlation id in events. It consists of a correlation id filter, a tap filter that always logs events with a correlation id to an appender, and a correlation id marker.","title":"Correlation ID"},{"location":"guide/correlationid/#correlation-id-filter","text":"A correlation id filter will filter for a correlation id set either as an MDC value, or as a marker created from CorrelationIdMarker . <appender name= \"LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > <filter class= \"com.tersesystems.logback.correlationid.CorrelationIdFilter\" > <mdcKey> correlationId </mdcKey> </filter> </appender> If an appender passes the filter, it will log the event. public class CorrelationIdFilterTest { public void testFilter () { // Write something that never gets logged explicitly... Logger logger = loggerFactory . getLogger ( \"com.example.Debug\" ); String correlationId = \"12345\" ; CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); // should be logged because marker logger . info ( correlationIdMarker , \"info one\" ); logger . info ( \"info two\" ); // should not be logged // Everything below this point should be logged. MDC . put ( \"correlationId\" , correlationId ); logger . info ( \"info three\" ); // should not be logged logger . info ( correlationIdMarker , \"info four\" ); } }","title":"Correlation ID Filter"},{"location":"guide/correlationid/#correlationidtapfilter","text":"The CorrelationIdTapFilter is a turbofilter that always logs to a given appender if the correlation id appears, even if the appender is not configured for logging. This functions as a wiretap . Tap Filters are very useful as a way to send data to an appender. They completely bypass any kind of logging level configured on the front end, so you can set a logger to INFO level but still have access to all TRACE events when an error occurs, through the tap filter's appenders. For example, a tap filter can automatically log everything with a correlation id at a TRACE level, without requiring filters or altering the log level as a whole. Let's run a simple HTTP client program that calls out to Google and prints a result. <configuration> <newRule pattern= \"configuration/turboFilter/appender-ref\" actionClass= \"ch.qos.logback.core.joran.action.AppenderRefAction\" /> <appender name= \"TAP_LIST\" class= \"ch.qos.logback.core.read.ListAppender\" > </appender> <turboFilter class= \"com.tersesystems.logback.correlationid.CorrelationIdTapFilter\" > <mdcKey> correlationId </mdcKey> <appender-ref ref= \"TAP_LIST\" /> </turboFilter> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration>","title":"CorrelationIdTapFilter"},{"location":"guide/correlationid/#correlationidmarker","text":"A CorrelationIdMarker implements the CorrelationIdProvider interface to expose a marker which is known to contain a correlation id. CorrelationIdMarker correlationIdMarker = CorrelationIdMarker . create ( correlationId ); String sameId = correlationIdMarker . getCorrelationId ();","title":"CorrelationIdMarker"},{"location":"guide/correlationid/#correlationidutils","text":"CorrelationIdUtils contains utility methods like get which retrieve a correlation id from either a marker or MDC.","title":"CorrelationIdUtils"},{"location":"guide/exception-mapping/","text":"Exception Mapping \u00b6 Exception Mapping is done to show the important details of an exception, including the root cause in a summary format. This is especially useful in line oriented formats, because rendering a stacktrace can take up screen real estate without providing much value. An example will help. Given the following program: public class Thrower { private static final Logger logger = LoggerFactory . getLogger ( Thrower . class ); public static void main ( String [] progArgs ) { try { doSomethingExceptional (); } catch ( RuntimeException e ) { logger . error ( \"domain specific message\" , e ); } } static void doSomethingExceptional () { Throwable cause = new BatchUpdateException (); throw new MyCustomException ( \"This is my message\" , \"one is one\" , \"two is more than one\" , \"three is more than two and one\" , cause ); } } public class MyCustomException extends RuntimeException { public MyCustomException ( String message , String one , String two , String three , Throwable cause ) { // ... } public String getOne () { return one ; } public String getTwo () { return two ; } public String getThree () { return three ; } } and the Logback file: <configuration> <newRule pattern= \"*/exceptionMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingRegistryAction\" /> <newRule pattern= \"*/exceptionMappings/mapping\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingAction\" /> <conversionRule conversionWord= \"richex\" converterClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMessageWithMappingsConverter\" /> <exceptionMappings> <!-- comes with default mappings for JDK exceptions, but you can add your own --> <mapping name= \"com.tersesystems.logback.exceptionmapping.MyCustomException\" properties= \"one,two,three\" /> </exceptionMappings> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%richex{1, 10, exception=[}%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> Then this renders the following: 184 ERROR c.t.l.exceptionmapping.Thrower - domain specific message exception=[com.tersesystems.logback.exceptionmapping.MyCustomException(one=\"one is one\" two=\"two is more than one\" three=\"three is more than two and one\" message=\"This is my message\") > java.sql.BatchUpdateException(updateCounts=\"null\" errorCode=\"0\" SQLState=\"null\" message=\"null\")] You can integrate exception mapping with Typesafe Config and logstash-logback-encoder by adding extra mappings. For example, you can map a whole bunch of exceptions at once in HOCON, and not have to do it line by line in XML: <configuration> <newRule pattern= \"*/exceptionMappings/configMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.config.TypesafeConfigMappingsAction\" /> <exceptionMappings> <!-- Or point to HOCON path --> <configMappings path= \"exceptionmappings\" /> </exceptionMappings> </configuration> and exceptionmappings { example.MySpecialException: [\"timestamp\"] } and configure it in JSON using ExceptionArgumentsProvider : <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <provider class= \"com.tersesystems.logback.exceptionmapping.json.ExceptionArgumentsProvider\" > <fieldName> exception </fieldName> </provider> </providers> </encoder> and get the following exception that contains an array of exceptions and the associated properties, in this case timestamp : { \"id\" : \"Fa6x8H0EqomdHaINzdiAAA\" , \"sequence\" : 3 , \"@timestamp\" : \"2019-07-06T03:52:48.730+00:00\" , \"@version\" : \"1\" , \"message\" : \"I am an error\" , \"logger_name\" : \"example.Main$Runner\" , \"thread_name\" : \"pool-1-thread-1\" , \"level\" : \"ERROR\" , \"stack_hash\" : \"233f3cf1\" , \"exception\" : [ { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 1\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 2\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 3\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 4\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 5\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 6\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 7\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 8\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 9\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } } ], \"stack_trace\" : \"<#1165e3b1> example.MySpecialException: Level 9\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 9 common frames omitted\\nWrapped by: <#eb336a2d> example.MySpecialException: Level 8\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 10 common frames omitted\\nWrapped by: <#cc1fb404> example.MySpecialException: Level 7\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 11 common frames omitted\\nWrapped by: <#2af187a0> example.MySpecialException: Level 6\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 12 common frames omitted\\nWrapped by: <#7dac62d1> example.MySpecialException: Level 5\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 13 common frames omitted\\nWrapped by: <#2ea4460d> example.MySpecialException: Level 4\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 14 common frames omitted\\nWrapped by: <#261bed64> example.MySpecialException: Level 3\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 15 common frames omitted\\nWrapped by: <#e660d440> example.MySpecialException: Level 2\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 16 common frames omitted\\nWrapped by: <#233f3cf1> example.MySpecialException: Level 1\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.generateException(Main.java:51)\\n\\tat example.Main$Runner.doError(Main.java:44)\\n\\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\n\\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\" } This is a lot easier for structured logging parsers to grok than the associated stacktrace. See How to Log an Exception and How to Log an Exception, Part 2 for more details.","title":"Exception Mapping"},{"location":"guide/exception-mapping/#exception-mapping","text":"Exception Mapping is done to show the important details of an exception, including the root cause in a summary format. This is especially useful in line oriented formats, because rendering a stacktrace can take up screen real estate without providing much value. An example will help. Given the following program: public class Thrower { private static final Logger logger = LoggerFactory . getLogger ( Thrower . class ); public static void main ( String [] progArgs ) { try { doSomethingExceptional (); } catch ( RuntimeException e ) { logger . error ( \"domain specific message\" , e ); } } static void doSomethingExceptional () { Throwable cause = new BatchUpdateException (); throw new MyCustomException ( \"This is my message\" , \"one is one\" , \"two is more than one\" , \"three is more than two and one\" , cause ); } } public class MyCustomException extends RuntimeException { public MyCustomException ( String message , String one , String two , String three , Throwable cause ) { // ... } public String getOne () { return one ; } public String getTwo () { return two ; } public String getThree () { return three ; } } and the Logback file: <configuration> <newRule pattern= \"*/exceptionMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingRegistryAction\" /> <newRule pattern= \"*/exceptionMappings/mapping\" actionClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMappingAction\" /> <conversionRule conversionWord= \"richex\" converterClass= \"com.tersesystems.logback.exceptionmapping.ExceptionMessageWithMappingsConverter\" /> <exceptionMappings> <!-- comes with default mappings for JDK exceptions, but you can add your own --> <mapping name= \"com.tersesystems.logback.exceptionmapping.MyCustomException\" properties= \"one,two,three\" /> </exceptionMappings> <appender name= \"CONSOLE\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%richex{1, 10, exception=[}%n </pattern> </encoder> </appender> <root level= \"TRACE\" > <appender-ref ref= \"CONSOLE\" /> </root> </configuration> Then this renders the following: 184 ERROR c.t.l.exceptionmapping.Thrower - domain specific message exception=[com.tersesystems.logback.exceptionmapping.MyCustomException(one=\"one is one\" two=\"two is more than one\" three=\"three is more than two and one\" message=\"This is my message\") > java.sql.BatchUpdateException(updateCounts=\"null\" errorCode=\"0\" SQLState=\"null\" message=\"null\")] You can integrate exception mapping with Typesafe Config and logstash-logback-encoder by adding extra mappings. For example, you can map a whole bunch of exceptions at once in HOCON, and not have to do it line by line in XML: <configuration> <newRule pattern= \"*/exceptionMappings/configMappings\" actionClass= \"com.tersesystems.logback.exceptionmapping.config.TypesafeConfigMappingsAction\" /> <exceptionMappings> <!-- Or point to HOCON path --> <configMappings path= \"exceptionmappings\" /> </exceptionMappings> </configuration> and exceptionmappings { example.MySpecialException: [\"timestamp\"] } and configure it in JSON using ExceptionArgumentsProvider : <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <provider class= \"com.tersesystems.logback.exceptionmapping.json.ExceptionArgumentsProvider\" > <fieldName> exception </fieldName> </provider> </providers> </encoder> and get the following exception that contains an array of exceptions and the associated properties, in this case timestamp : { \"id\" : \"Fa6x8H0EqomdHaINzdiAAA\" , \"sequence\" : 3 , \"@timestamp\" : \"2019-07-06T03:52:48.730+00:00\" , \"@version\" : \"1\" , \"message\" : \"I am an error\" , \"logger_name\" : \"example.Main$Runner\" , \"thread_name\" : \"pool-1-thread-1\" , \"level\" : \"ERROR\" , \"stack_hash\" : \"233f3cf1\" , \"exception\" : [ { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 1\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 2\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 3\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 4\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 5\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 6\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 7\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 8\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } }, { \"name\" : \"example.MySpecialException\" , \"properties\" : { \"message\" : \"Level 9\" , \"timestamp\" : \"2019-07-06T03:52:48.728Z\" } } ], \"stack_trace\" : \"<#1165e3b1> example.MySpecialException: Level 9\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 9 common frames omitted\\nWrapped by: <#eb336a2d> example.MySpecialException: Level 8\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 10 common frames omitted\\nWrapped by: <#cc1fb404> example.MySpecialException: Level 7\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 11 common frames omitted\\nWrapped by: <#2af187a0> example.MySpecialException: Level 6\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 12 common frames omitted\\nWrapped by: <#7dac62d1> example.MySpecialException: Level 5\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 13 common frames omitted\\nWrapped by: <#2ea4460d> example.MySpecialException: Level 4\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 14 common frames omitted\\nWrapped by: <#261bed64> example.MySpecialException: Level 3\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 15 common frames omitted\\nWrapped by: <#e660d440> example.MySpecialException: Level 2\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\t... 16 common frames omitted\\nWrapped by: <#233f3cf1> example.MySpecialException: Level 1\\n\\tat example.Main$Runner.nestException(Main.java:56)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.nestException(Main.java:57)\\n\\tat example.Main$Runner.generateException(Main.java:51)\\n\\tat example.Main$Runner.doError(Main.java:44)\\n\\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\n\\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\\n\\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\" } This is a lot easier for structured logging parsers to grok than the associated stacktrace. See How to Log an Exception and How to Log an Exception, Part 2 for more details.","title":"Exception Mapping"},{"location":"guide/instrumentation/","text":"Instrumentation \u00b6 If you have library code that doesn't pass around ILoggerFactory and doesn't let you add information to logging, then you can get around this by instrumenting the code with Byte Buddy . Using Byte Buddy, you can do fun things like override Security.setSystemManager with your own implementation , so using Byte Buddy to decorate code with enter and exit logging statements is relatively straightforward. Instrumentation is configuration driven and simple. Instead of debugging using printf statements and recompiling or stepping through a debugger, you can just add lines to a config file. I like this approach better than the annotation or aspect-oriented programming approaches, because it is completely transparent to the code and gives roughly the same performance as inline code, adding 130 ns/op by calling class.getMethod . A major advantage of instrumentation is that because it logs throwing exceptions in instrumented code, you can log exceptions that would be swallowed by the caller. For example, imagine that a library has the following method: public class Foo { public void throwException () throws Exception { throw new PlumException ( \"I am sweet and cold\" ); } public void swallowException () { try { throwException (); } catch ( Exception e ) { // forgive me, the exception was delicious } } } By instrumenting the throwException method, you can see the logged exception at runtime when swallowException is called. See Application Logging in Java: Tracing 3 rd Party Code and Hierarchical Instrumented Tracing with Logback for more details. Installation \u00b6 You'll need to install logback-bytebuddy and logback-tracing , and provide a byte-buddy implementation. implementation group: 'com.tersesystems.logback', name: 'logback-classic', version: 'LATEST' implementation group: 'com.tersesystems.logback', name: 'logback-bytebuddy', version: 'LATEST' implementation group: 'com.tersesystems.logback', name: 'logback-tracing', version: 'LATEST' implementation group: 'net.bytebuddy', name: 'byte-buddy', version: 'LATEST' There are two ways you can install instrumentation -- you can do it using an agent, or you can do it manually. NOTE: Because Byte Buddy must inspect each class on JVM initialization, it will have a (generally small) impact on the start up time of your application. Agent Installation \u00b6 Using the agent is generally easier (less code) and more powerful (can change JDK classes), but it does require some explicit command line options. First, you set the java agent, either directly on the command line: java \\ -javaagent:path/to/logback-bytebuddy-x.x.x.jar = debug \\ -Dterse.logback.configurationFile = conf/logback.conf \\ -Dlogback.configurationFile = conf/logback-test.xml \\ com.example.PreloadedInstrumentationExample or by using the JAVA_TOOLS_OPTIONS environment variable . export JAVA_TOOLS_OPTIONS = \"...\" Generally you'll be setting up these options in a build system. There are example projects in Gradle and sbt set up with agent-based instrumentation at https://github.com/tersesystems/logging-instrumentation-example . Manual Installation \u00b6 You also have the option of installing the agent manually. The in process instrumentation is done with com.tersesystems.logback.bytebuddy.LoggingInstrumentationByteBuddyBuilder , which takes in some configuration and then installs itself on the byte buddy agent. new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( loggingInstrumentationAdviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent (); Configuration \u00b6 There are two parts to seeing tracing logs with instrumentation -- indicating the classes and methods you want instrumented, and then setting those loggers to TRACE. Setting Instrumented Classes and Methods \u00b6 The instrumentation is configured using HOCON in a logback.conf file in src/main/resources . Settings are under the logback.bytebuddy section. The tracing section contains a mapping of class names and methods, or the wildcard \"*\" to indicate all methods. logback.bytebuddy { service-name = \"my-service\" tracing { \"fully.qualified.class.Name\" = [\"method1\", \"method2\"] \"play.api.mvc.ActionBuilder\" = [\"*\"] } } NOTE: There are some limitations to what you can trace. You can only instrument JDK classes when using the agent, and you cannot instrument native methods like java.lang.System.currentTimeMillis() for example. Setting Loggers to TRACE \u00b6 Because instrumentation inserts logger.trace calls into the code, you must enable logging at TRACE level for those loggers to see output. Setting the level from logback.xml works fine: <configuration> <!-- ... --> <logger name= \"fully.qualified.class.Name\" level= \"TRACE\" /> <logger name= \"play.api.mvc.ActionBuilder\" level= \"TRACE\" /> <!-- ... --> </configuration> If you are using the Config module, you can also do this from logback.conf : levels { fully.qualified.class.Name = TRACE play.api.mvc.ActionBuilder = TRACE } Or you can use ChangeLogLevel at run time. Examples \u00b6 Instrumentation is a tool that can be hard to explain, so here's some use cases showing how you can quickly instrument your code. Also don't forget the example projects at https://github.com/tersesystems/logging-instrumentation-example . Instrumenting java.lang.Thread \u00b6 Assuming an agent based instrumentation, in logback.conf : levels { java.lang.Thread = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"java.lang.Thread\" = [ \"run\" ] } } and the code as follows: public class PreloadedInstrumentationExample { public static void main ( String [] args ) throws Exception { Thread thread = Thread . currentThread (); thread . run (); } } yields [Byte Buddy] DISCOVERY java.lang.Thread [null, null, loaded=true] [Byte Buddy] TRANSFORM java.lang.Thread [null, null, loaded=true] [Byte Buddy] COMPLETE java.lang.Thread [null, null, loaded=true] 92 TRACE java.lang.Thread - entering: java.lang.Thread.run() with arguments=[] 93 TRACE java.lang.Thread - exiting: java.lang.Thread.run() with arguments=[] => returnType=void Instrumenting javax.net.ssl.SSLContext \u00b6 This is especially helpful when you're trying to debug SSL issues: levels { sun.security.ssl = TRACE javax.net.ssl = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"javax.net.ssl.SSLContext\" = [\"*\"] } } will result in: FcJ3XfsdKnM6O0Qbm7EAAA 12:31:55.498 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] from source SSLContext.java:155 FcJ3XfsdKng6O0Qbm7EAAA 12:31:55.503 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] => returnType=javax.net.ssl.SSLContext from source SSLContext.java:157 FcJ3XfsdKng6O0Qbm7EAAB 12:31:55.504 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] from source SSLContext.java:282 FcJ3XfsdKnk6O0Qbm7EAAA 12:31:55.504 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] => returnType=void from source SSLContext.java:283 Be warned that JSSE can be extremely verbose in its toString output. Instrumenting ClassCalledByAgent \u00b6 If you are already developing an agent, or want finer grained control over Byte Buddy, you can create the agent in process and inspect how Byte Buddy works. This is an advanced use case, but it's useful to get familiar. With the following code: public class ClassCalledByAgent { public void printStatement () { System . out . println ( \"I am a simple println method with no logging\" ); } public void printArgument ( String arg ) { System . out . println ( \"I am a simple println, printing \" + arg ); } public void throwException ( String arg ) { throw new RuntimeException ( \"I'm a squirrel!\" ); } } And the following configuration in logback.conf : logback.bytebuddy { service-name = \"example-service\" tracing { \"com.tersesystems.logback.bytebuddy.ClassCalledByAgent\" = [ \"printStatement\", \"printArgument\", \"throwException\", ] } } and have com.tersesystems.logback.bytebuddy.ClassCalledByAgent logging level set to TRACE in logback.xml . We can start up the agent, add in the builder and run through the methods: public class InProcessInstrumentationExample { public static AgentBuilder . Listener createDebugListener ( List < String > classNames ) { return new AgentBuilder . Listener . Filtering ( LoggingInstrumentationAdvice . stringMatcher ( classNames ), AgentBuilder . Listener . StreamWriting . toSystemOut ()); } public static void main ( String [] args ) throws Exception { // Helps if you install the byte buddy agents before anything else at all happens... ByteBuddyAgent . install (); Logger logger = LoggerFactory . getLogger ( InProcessInstrumentationExample . class ); SystemFlow . setLoggerResolver ( new FixedLoggerResolver ( logger )); Config config = LoggingInstrumentationAdvice . generateConfig ( ClassLoader . getSystemClassLoader (), false ); LoggingInstrumentationAdviceConfig adviceConfig = LoggingInstrumentationAdvice . generateAdviceConfig ( config ); // The debugging listener shows what classes are being picked up by the instrumentation Listener debugListener = createDebugListener ( adviceConfig . classNames ()); new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( adviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent (); // No code change necessary here, you can wrap completely in the agent... ClassCalledByAgent classCalledByAgent = new ClassCalledByAgent (); classCalledByAgent . printStatement (); classCalledByAgent . printArgument ( \"42\" ); try { classCalledByAgent . throwException ( \"hello world\" ); } catch ( Exception e ) { // I am too lazy to catch this exception. I hope someone does it for me. } } } And get the following: [Byte Buddy] DISCOVERY com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] TRANSFORM com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] COMPLETE com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] 524 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] from source ClassCalledByAgent.java:18 I am a simple println method with no logging 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] => returnType=void from source ClassCalledByAgent.java:19 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] from source ClassCalledByAgent.java:22 I am a simple println, printing 42 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] => returnType=void from source ClassCalledByAgent.java:23 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] from source ClassCalledByAgent.java:26 532 ERROR c.t.l.b.InProcessInstrumentationExample - throwing: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] ! thrown=java.lang.RuntimeException: I'm a squirrel! java.lang.RuntimeException: I'm a squirrel! at com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(ClassCalledByAgent.java:26) at com.tersesystems.logback.bytebuddy.InProcessInstrumentationExample.main(InProcessInstrumentationExample.java:65) The [Byte Buddy] statements up top are caused by the debug listener, and let you know that Byte Buddy has successfully instrumented the class. Note also that there is no runtime overhead in pulling line numbers or source files into the enter/exit methods, as these are pulled directly from bytecode and do not involve fillInStackTrace .","title":"Instrumentation"},{"location":"guide/instrumentation/#instrumentation","text":"If you have library code that doesn't pass around ILoggerFactory and doesn't let you add information to logging, then you can get around this by instrumenting the code with Byte Buddy . Using Byte Buddy, you can do fun things like override Security.setSystemManager with your own implementation , so using Byte Buddy to decorate code with enter and exit logging statements is relatively straightforward. Instrumentation is configuration driven and simple. Instead of debugging using printf statements and recompiling or stepping through a debugger, you can just add lines to a config file. I like this approach better than the annotation or aspect-oriented programming approaches, because it is completely transparent to the code and gives roughly the same performance as inline code, adding 130 ns/op by calling class.getMethod . A major advantage of instrumentation is that because it logs throwing exceptions in instrumented code, you can log exceptions that would be swallowed by the caller. For example, imagine that a library has the following method: public class Foo { public void throwException () throws Exception { throw new PlumException ( \"I am sweet and cold\" ); } public void swallowException () { try { throwException (); } catch ( Exception e ) { // forgive me, the exception was delicious } } } By instrumenting the throwException method, you can see the logged exception at runtime when swallowException is called. See Application Logging in Java: Tracing 3 rd Party Code and Hierarchical Instrumented Tracing with Logback for more details.","title":"Instrumentation"},{"location":"guide/instrumentation/#installation","text":"You'll need to install logback-bytebuddy and logback-tracing , and provide a byte-buddy implementation. implementation group: 'com.tersesystems.logback', name: 'logback-classic', version: 'LATEST' implementation group: 'com.tersesystems.logback', name: 'logback-bytebuddy', version: 'LATEST' implementation group: 'com.tersesystems.logback', name: 'logback-tracing', version: 'LATEST' implementation group: 'net.bytebuddy', name: 'byte-buddy', version: 'LATEST' There are two ways you can install instrumentation -- you can do it using an agent, or you can do it manually. NOTE: Because Byte Buddy must inspect each class on JVM initialization, it will have a (generally small) impact on the start up time of your application.","title":"Installation"},{"location":"guide/instrumentation/#agent-installation","text":"Using the agent is generally easier (less code) and more powerful (can change JDK classes), but it does require some explicit command line options. First, you set the java agent, either directly on the command line: java \\ -javaagent:path/to/logback-bytebuddy-x.x.x.jar = debug \\ -Dterse.logback.configurationFile = conf/logback.conf \\ -Dlogback.configurationFile = conf/logback-test.xml \\ com.example.PreloadedInstrumentationExample or by using the JAVA_TOOLS_OPTIONS environment variable . export JAVA_TOOLS_OPTIONS = \"...\" Generally you'll be setting up these options in a build system. There are example projects in Gradle and sbt set up with agent-based instrumentation at https://github.com/tersesystems/logging-instrumentation-example .","title":"Agent Installation"},{"location":"guide/instrumentation/#manual-installation","text":"You also have the option of installing the agent manually. The in process instrumentation is done with com.tersesystems.logback.bytebuddy.LoggingInstrumentationByteBuddyBuilder , which takes in some configuration and then installs itself on the byte buddy agent. new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( loggingInstrumentationAdviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent ();","title":"Manual Installation"},{"location":"guide/instrumentation/#configuration","text":"There are two parts to seeing tracing logs with instrumentation -- indicating the classes and methods you want instrumented, and then setting those loggers to TRACE.","title":"Configuration"},{"location":"guide/instrumentation/#setting-instrumented-classes-and-methods","text":"The instrumentation is configured using HOCON in a logback.conf file in src/main/resources . Settings are under the logback.bytebuddy section. The tracing section contains a mapping of class names and methods, or the wildcard \"*\" to indicate all methods. logback.bytebuddy { service-name = \"my-service\" tracing { \"fully.qualified.class.Name\" = [\"method1\", \"method2\"] \"play.api.mvc.ActionBuilder\" = [\"*\"] } } NOTE: There are some limitations to what you can trace. You can only instrument JDK classes when using the agent, and you cannot instrument native methods like java.lang.System.currentTimeMillis() for example.","title":"Setting Instrumented Classes and Methods"},{"location":"guide/instrumentation/#setting-loggers-to-trace","text":"Because instrumentation inserts logger.trace calls into the code, you must enable logging at TRACE level for those loggers to see output. Setting the level from logback.xml works fine: <configuration> <!-- ... --> <logger name= \"fully.qualified.class.Name\" level= \"TRACE\" /> <logger name= \"play.api.mvc.ActionBuilder\" level= \"TRACE\" /> <!-- ... --> </configuration> If you are using the Config module, you can also do this from logback.conf : levels { fully.qualified.class.Name = TRACE play.api.mvc.ActionBuilder = TRACE } Or you can use ChangeLogLevel at run time.","title":"Setting Loggers to TRACE"},{"location":"guide/instrumentation/#examples","text":"Instrumentation is a tool that can be hard to explain, so here's some use cases showing how you can quickly instrument your code. Also don't forget the example projects at https://github.com/tersesystems/logging-instrumentation-example .","title":"Examples"},{"location":"guide/instrumentation/#instrumenting-javalangthread","text":"Assuming an agent based instrumentation, in logback.conf : levels { java.lang.Thread = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"java.lang.Thread\" = [ \"run\" ] } } and the code as follows: public class PreloadedInstrumentationExample { public static void main ( String [] args ) throws Exception { Thread thread = Thread . currentThread (); thread . run (); } } yields [Byte Buddy] DISCOVERY java.lang.Thread [null, null, loaded=true] [Byte Buddy] TRANSFORM java.lang.Thread [null, null, loaded=true] [Byte Buddy] COMPLETE java.lang.Thread [null, null, loaded=true] 92 TRACE java.lang.Thread - entering: java.lang.Thread.run() with arguments=[] 93 TRACE java.lang.Thread - exiting: java.lang.Thread.run() with arguments=[] => returnType=void","title":"Instrumenting java.lang.Thread"},{"location":"guide/instrumentation/#instrumenting-javaxnetsslsslcontext","text":"This is especially helpful when you're trying to debug SSL issues: levels { sun.security.ssl = TRACE javax.net.ssl = TRACE } logback.bytebuddy { service-name = \"some-service\" tracing { \"javax.net.ssl.SSLContext\" = [\"*\"] } } will result in: FcJ3XfsdKnM6O0Qbm7EAAA 12:31:55.498 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] from source SSLContext.java:155 FcJ3XfsdKng6O0Qbm7EAAA 12:31:55.503 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.getInstance(java.lang.String) with arguments=[TLS] => returnType=javax.net.ssl.SSLContext from source SSLContext.java:157 FcJ3XfsdKng6O0Qbm7EAAB 12:31:55.504 [TRACE] j.n.s.SSLContext - entering: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] from source SSLContext.java:282 FcJ3XfsdKnk6O0Qbm7EAAA 12:31:55.504 [TRACE] j.n.s.SSLContext - exiting: javax.net.ssl.SSLContext.init([Ljavax.net.ssl.KeyManager;,[Ljavax.net.ssl.TrustManager;,java.security.SecureRandom) with arguments=[[org.postgresql.ssl.LazyKeyManager@27a97e08], [org.postgresql.ssl.NonValidatingFactory$NonValidatingTM@5918c260], null] => returnType=void from source SSLContext.java:283 Be warned that JSSE can be extremely verbose in its toString output.","title":"Instrumenting javax.net.ssl.SSLContext"},{"location":"guide/instrumentation/#instrumenting-classcalledbyagent","text":"If you are already developing an agent, or want finer grained control over Byte Buddy, you can create the agent in process and inspect how Byte Buddy works. This is an advanced use case, but it's useful to get familiar. With the following code: public class ClassCalledByAgent { public void printStatement () { System . out . println ( \"I am a simple println method with no logging\" ); } public void printArgument ( String arg ) { System . out . println ( \"I am a simple println, printing \" + arg ); } public void throwException ( String arg ) { throw new RuntimeException ( \"I'm a squirrel!\" ); } } And the following configuration in logback.conf : logback.bytebuddy { service-name = \"example-service\" tracing { \"com.tersesystems.logback.bytebuddy.ClassCalledByAgent\" = [ \"printStatement\", \"printArgument\", \"throwException\", ] } } and have com.tersesystems.logback.bytebuddy.ClassCalledByAgent logging level set to TRACE in logback.xml . We can start up the agent, add in the builder and run through the methods: public class InProcessInstrumentationExample { public static AgentBuilder . Listener createDebugListener ( List < String > classNames ) { return new AgentBuilder . Listener . Filtering ( LoggingInstrumentationAdvice . stringMatcher ( classNames ), AgentBuilder . Listener . StreamWriting . toSystemOut ()); } public static void main ( String [] args ) throws Exception { // Helps if you install the byte buddy agents before anything else at all happens... ByteBuddyAgent . install (); Logger logger = LoggerFactory . getLogger ( InProcessInstrumentationExample . class ); SystemFlow . setLoggerResolver ( new FixedLoggerResolver ( logger )); Config config = LoggingInstrumentationAdvice . generateConfig ( ClassLoader . getSystemClassLoader (), false ); LoggingInstrumentationAdviceConfig adviceConfig = LoggingInstrumentationAdvice . generateAdviceConfig ( config ); // The debugging listener shows what classes are being picked up by the instrumentation Listener debugListener = createDebugListener ( adviceConfig . classNames ()); new LoggingInstrumentationByteBuddyBuilder () . builderFromConfig ( adviceConfig ) . with ( debugListener ) . installOnByteBuddyAgent (); // No code change necessary here, you can wrap completely in the agent... ClassCalledByAgent classCalledByAgent = new ClassCalledByAgent (); classCalledByAgent . printStatement (); classCalledByAgent . printArgument ( \"42\" ); try { classCalledByAgent . throwException ( \"hello world\" ); } catch ( Exception e ) { // I am too lazy to catch this exception. I hope someone does it for me. } } } And get the following: [Byte Buddy] DISCOVERY com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] TRANSFORM com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] [Byte Buddy] COMPLETE com.tersesystems.logback.bytebuddy.ClassCalledByAgent [sun.misc.Launcher$AppClassLoader@75b84c92, null, loaded=true] 524 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] from source ClassCalledByAgent.java:18 I am a simple println method with no logging 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printStatement() with arguments=[] => returnType=void from source ClassCalledByAgent.java:19 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] from source ClassCalledByAgent.java:22 I am a simple println, printing 42 529 TRACE c.t.l.b.InProcessInstrumentationExample - exiting: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.printArgument(java.lang.String) with arguments=[42] => returnType=void from source ClassCalledByAgent.java:23 529 TRACE c.t.l.b.InProcessInstrumentationExample - entering: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] from source ClassCalledByAgent.java:26 532 ERROR c.t.l.b.InProcessInstrumentationExample - throwing: com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(java.lang.String) with arguments=[hello world] ! thrown=java.lang.RuntimeException: I'm a squirrel! java.lang.RuntimeException: I'm a squirrel! at com.tersesystems.logback.bytebuddy.ClassCalledByAgent.throwException(ClassCalledByAgent.java:26) at com.tersesystems.logback.bytebuddy.InProcessInstrumentationExample.main(InProcessInstrumentationExample.java:65) The [Byte Buddy] statements up top are caused by the debug listener, and let you know that Byte Buddy has successfully instrumented the class. Note also that there is no runtime overhead in pulling line numbers or source files into the enter/exit methods, as these are pulled directly from bytecode and do not involve fillInStackTrace .","title":"Instrumenting ClassCalledByAgent"},{"location":"guide/jdbc/","text":"JDBC \u00b6 There is a JDBC appender included which can be subclassed and extended as necessary in the logback-jdbc-appender module. Using a database for logging can be a big help when you just want to get at the logs of the last 30 seconds from inside the application. Because JDBC is both accessible and understandable, there's very little work required for querying. Logback does have a native JDBC appender, but unfortunately it requires three tables and is not set up for easy subclassing. This one is better. This implementation assumes a single table, with a user defined extensible schema, and is set up with HikariCP and a thread pool executor to serve JDBC with minimal blocking. Note that you should always use a JDBC appender behind an LoggingEventAsyncDisruptorAppender and you should have an appropriately sized connection pool for your database traffic. Database timestamps record time with microsecond resolution, whereas millisecond resolution is commonplace for logging, so for convenience both the timestamp with time zone and the time since epoch are recorded. For span information, the start time must also be recorded as TSE. Likewise, the level is recorded as both a text string for visual reference, and a level value so that you can order and filter database queries. Querying a database can be helpful when errors occur, because you can pull out all logs with a correlation id. See the logback-correlationid module for an example. Logging using in-memory H2 Database \u00b6 Using an in memory H2 database is a cheap and easy way to expose logs from inside the application without having to parse files. <appender name= \"H2_JDBC\" class= \"com.tersesystems.logback.jdbc.JDBCAppender\" > <driver> jdbc:h2:mem:logback </driver> <url> org.h2.Driver </url> <username> sa </username> <password></password> <createStatements> CREATE TABLE IF NOT EXISTS events ( ID INT NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, tse_ms numeric NOT NULL, start_ms numeric NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL ); </createStatements> <insertStatement> insert into events(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <reaperStatement> delete from events where ts &lt; ? </reaperStatement> <reaperSchedule> PT30 </reaperSchedule> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender> Logging using PostgresSQL \u00b6 If you want something larger scale, you'll probably be using Postgres instead of H2. You can log JSON to PostgreSQL, using the built-in JSON datatype . Postgres uses a custom JDBC type of PGObject , so the insertEvent method must be subclassed. This is what's in the logback-postgresjson-appender module: public class PostgresJsonAppender extends JDBCAppender { private String objectType = \"json\" ; public String getObjectType () { return objectType ; } public void setObjectType ( String objectType ) { this . objectType = objectType ; } @Override public void start () { super . start (); setDriver ( \"org.postgresql.Driver\" ); } @Override protected void insertEvent ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { PGobject jsonObject = new PGobject (); jsonObject . setType ( getObjectType ()); byte [] bytes = getEncoder (). encode ( event ); jsonObject . setValue ( new String ( bytes , StandardCharsets . UTF_8 )); statement . setObject ( adder . intValue (), jsonObject ); adder . increment (); } } First, install PostgreSQL, create a database logback , a role logback and a password logback and add the following table: CREATE TABLE logging_table ( ID serial NOT NULL PRIMARY KEY , ts TIMESTAMPTZ ( 6 ) NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin ( evt ); Because logs are inherently time-series data, you can use the timescaleDB postgresql extension as described in Store application logs in timescaleDB/postgres , but that's not required. Then, add the following logback.xml : <configuration> <!-- async appender needs a shutdown hook to make sure this clears --> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" /> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <!-- SQL is blocking, so use an async lmax appender here --> <appender name= \"ASYNC_POSTGRES\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.postgresjson.PostgresJsonAppender\" > <createStatements> CREATE TABLE IF NOT EXISTS logging_table ( ID serial NOT NULL PRIMARY KEY, ts TIMESTAMPTZ(6) NOT NULL, tse_ms bigint NOT NULL, start_ms bigint NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin (evt); </createStatements> <!-- SQL statement takes a TIMESTAMP, LONG, INT, VARCHAR, PGObject --> <insertStatement> insert into logging_table(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <url> jdbc:postgresql://localhost:5432/logback </url> <username> logback </username> <password> logback </password> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"ASYNC_POSTGRES\" /> <appender-ref ref= \"STDOUT\" /> </root> </configuration> Querying requires a little bit of extra syntax, using evt->'myfield' to select: select ts as end_date , start_ms as epoch_start , tse_ms as epoch_end , evt -> 'trace.span_id' as span_id , evt -> 'name' as name , evt -> 'message' as message , evt -> 'trace.parent_id' as parent , evt -> 'duration_ms' as duration_ms from logging_table where evt -> 'trace.trace_id' IS NOT NULL order by ts desc limit 5 If you have extra logs that you want to import into PostgreSQL, you can use PSQL to do that . Extending JDBC Appender with extra fields \u00b6 The JDBC appender can be extended so you can add extra information to the table. In the logback-correlationid module, there's a CorrelationIdJdbcAppender that adds extra information into the event so you can query by the correlation id specifically, by using the insertAdditionalData hook: public class CorrelationIdJdbcAppender extends JDBCAppender { private String mdcKey = \"correlation_id\" ; public String getMdcKey () { return mdcKey ; } public void setMdcKey ( String mdcKey ) { this . mdcKey = mdcKey ; } protected CorrelationIdUtils utils ; @Override public void start () { super . start (); utils = new CorrelationIdUtils ( mdcKey ); } @Override protected void insertAdditionalData ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { insertCorrelationId ( event , adder , statement ); } private void insertCorrelationId ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { Optional < String > maybeCorrelationId = utils . get ( event . getMarker ()); if ( maybeCorrelationId . isPresent ()) { statement . setString ( adder . intValue (), maybeCorrelationId . get ()); } else { statement . setNull ( adder . intValue (), Types . VARCHAR ); } adder . increment (); } } Then set up the table and add an index on the correlation id: CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT , ts TIMESTAMP ( 9 ) WITH TIME ZONE NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt JSON NOT NULL , correlation_id VARCHAR ( 255 ) NULL ); CREATE INDEX correlation_id_idx ON events ( correlation_id ); And then you can query from there. See Logging Structured Data to Database for more details.","title":"JDBC"},{"location":"guide/jdbc/#jdbc","text":"There is a JDBC appender included which can be subclassed and extended as necessary in the logback-jdbc-appender module. Using a database for logging can be a big help when you just want to get at the logs of the last 30 seconds from inside the application. Because JDBC is both accessible and understandable, there's very little work required for querying. Logback does have a native JDBC appender, but unfortunately it requires three tables and is not set up for easy subclassing. This one is better. This implementation assumes a single table, with a user defined extensible schema, and is set up with HikariCP and a thread pool executor to serve JDBC with minimal blocking. Note that you should always use a JDBC appender behind an LoggingEventAsyncDisruptorAppender and you should have an appropriately sized connection pool for your database traffic. Database timestamps record time with microsecond resolution, whereas millisecond resolution is commonplace for logging, so for convenience both the timestamp with time zone and the time since epoch are recorded. For span information, the start time must also be recorded as TSE. Likewise, the level is recorded as both a text string for visual reference, and a level value so that you can order and filter database queries. Querying a database can be helpful when errors occur, because you can pull out all logs with a correlation id. See the logback-correlationid module for an example.","title":"JDBC"},{"location":"guide/jdbc/#logging-using-in-memory-h2-database","text":"Using an in memory H2 database is a cheap and easy way to expose logs from inside the application without having to parse files. <appender name= \"H2_JDBC\" class= \"com.tersesystems.logback.jdbc.JDBCAppender\" > <driver> jdbc:h2:mem:logback </driver> <url> org.h2.Driver </url> <username> sa </username> <password></password> <createStatements> CREATE TABLE IF NOT EXISTS events ( ID INT NOT NULL PRIMARY KEY AUTO_INCREMENT, ts TIMESTAMP(9) WITH TIME ZONE NOT NULL, tse_ms numeric NOT NULL, start_ms numeric NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt JSON NOT NULL ); </createStatements> <insertStatement> insert into events(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <reaperStatement> delete from events where ts &lt; ? </reaperStatement> <reaperSchedule> PT30 </reaperSchedule> <encoder class= \"net.logstash.logback.encoder.LogstashEncoder\" > </encoder> </appender>","title":"Logging using in-memory H2 Database"},{"location":"guide/jdbc/#logging-using-postgressql","text":"If you want something larger scale, you'll probably be using Postgres instead of H2. You can log JSON to PostgreSQL, using the built-in JSON datatype . Postgres uses a custom JDBC type of PGObject , so the insertEvent method must be subclassed. This is what's in the logback-postgresjson-appender module: public class PostgresJsonAppender extends JDBCAppender { private String objectType = \"json\" ; public String getObjectType () { return objectType ; } public void setObjectType ( String objectType ) { this . objectType = objectType ; } @Override public void start () { super . start (); setDriver ( \"org.postgresql.Driver\" ); } @Override protected void insertEvent ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { PGobject jsonObject = new PGobject (); jsonObject . setType ( getObjectType ()); byte [] bytes = getEncoder (). encode ( event ); jsonObject . setValue ( new String ( bytes , StandardCharsets . UTF_8 )); statement . setObject ( adder . intValue (), jsonObject ); adder . increment (); } } First, install PostgreSQL, create a database logback , a role logback and a password logback and add the following table: CREATE TABLE logging_table ( ID serial NOT NULL PRIMARY KEY , ts TIMESTAMPTZ ( 6 ) NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin ( evt ); Because logs are inherently time-series data, you can use the timescaleDB postgresql extension as described in Store application logs in timescaleDB/postgres , but that's not required. Then, add the following logback.xml : <configuration> <!-- async appender needs a shutdown hook to make sure this clears --> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" /> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <!-- SQL is blocking, so use an async lmax appender here --> <appender name= \"ASYNC_POSTGRES\" class= \"net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender\" > <appender class= \"com.tersesystems.logback.postgresjson.PostgresJsonAppender\" > <createStatements> CREATE TABLE IF NOT EXISTS logging_table ( ID serial NOT NULL PRIMARY KEY, ts TIMESTAMPTZ(6) NOT NULL, tse_ms bigint NOT NULL, start_ms bigint NULL, level_value int NOT NULL, level VARCHAR(7) NOT NULL, evt jsonb NOT NULL ); CREATE INDEX idxgin ON logging_table USING gin (evt); </createStatements> <!-- SQL statement takes a TIMESTAMP, LONG, INT, VARCHAR, PGObject --> <insertStatement> insert into logging_table(ts, tse_ms, start_ms, level_value, level, evt) values(?, ?, ?, ?, ?, ?) </insertStatement> <url> jdbc:postgresql://localhost:5432/logback </url> <username> logback </username> <password> logback </password> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> </appender> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <root level= \"INFO\" > <appender-ref ref= \"ASYNC_POSTGRES\" /> <appender-ref ref= \"STDOUT\" /> </root> </configuration> Querying requires a little bit of extra syntax, using evt->'myfield' to select: select ts as end_date , start_ms as epoch_start , tse_ms as epoch_end , evt -> 'trace.span_id' as span_id , evt -> 'name' as name , evt -> 'message' as message , evt -> 'trace.parent_id' as parent , evt -> 'duration_ms' as duration_ms from logging_table where evt -> 'trace.trace_id' IS NOT NULL order by ts desc limit 5 If you have extra logs that you want to import into PostgreSQL, you can use PSQL to do that .","title":"Logging using PostgresSQL"},{"location":"guide/jdbc/#extending-jdbc-appender-with-extra-fields","text":"The JDBC appender can be extended so you can add extra information to the table. In the logback-correlationid module, there's a CorrelationIdJdbcAppender that adds extra information into the event so you can query by the correlation id specifically, by using the insertAdditionalData hook: public class CorrelationIdJdbcAppender extends JDBCAppender { private String mdcKey = \"correlation_id\" ; public String getMdcKey () { return mdcKey ; } public void setMdcKey ( String mdcKey ) { this . mdcKey = mdcKey ; } protected CorrelationIdUtils utils ; @Override public void start () { super . start (); utils = new CorrelationIdUtils ( mdcKey ); } @Override protected void insertAdditionalData ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { insertCorrelationId ( event , adder , statement ); } private void insertCorrelationId ( ILoggingEvent event , LongAdder adder , PreparedStatement statement ) throws SQLException { Optional < String > maybeCorrelationId = utils . get ( event . getMarker ()); if ( maybeCorrelationId . isPresent ()) { statement . setString ( adder . intValue (), maybeCorrelationId . get ()); } else { statement . setNull ( adder . intValue (), Types . VARCHAR ); } adder . increment (); } } Then set up the table and add an index on the correlation id: CREATE TABLE IF NOT EXISTS events ( ID NUMERIC NOT NULL PRIMARY KEY AUTO_INCREMENT , ts TIMESTAMP ( 9 ) WITH TIME ZONE NOT NULL , tse_ms numeric NOT NULL , start_ms numeric NULL , level_value int NOT NULL , level VARCHAR ( 7 ) NOT NULL , evt JSON NOT NULL , correlation_id VARCHAR ( 255 ) NULL ); CREATE INDEX correlation_id_idx ON events ( correlation_id ); And then you can query from there. See Logging Structured Data to Database for more details.","title":"Extending JDBC Appender with extra fields"},{"location":"guide/relativens/","text":"Relative Nanoseconds Appender \u00b6 LoggingEvent already has a timestamp associated with it, but that timestamp is generated by System.currentTimeMillis . When your logging is fast enough that you can log several statements in the same millisecond, it can be frustrating to not know which came first. Adding a relative_ns field provides resolution down to the nanosecond, sort of . For example, here's two different records with the same millisecond. { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAA\" , \"relative_ns\" : 11808036 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"HikariPool-2 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAB\" , \"relative_ns\" : 11981656 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"jdbc-appender-pool-1584163602961 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"logback-appender-ASYNC_JDBC-2\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } Note that the timestamp is 2020-03-14T05:26:43.315Z and the time since epoch is 1584163603315 . The flake ids distinguish between log entries by using a counter when millisecond precision is exceeded, so the first record is FfwJtsNLLWo6O0Qbm7EAAA ending in A and the second record is FfwJtsNLLWo6O0Qbm7EAAB ending in B . The relative time tells us exactly how much time has elapsed between the two events: 11981656 - 11808036 is 0.17362 milliseconds. All logging events are computed using System.nanoTime - NanoTime.start , where NanoTime.start is a static final field initialized JVM start time (technically at class loading but close enough). This value may be negative to begin with, but always increments. See the showcase for an example. Usage \u00b6 <appender class= \"com.tersesystems.logback.classic.NanoTimeComponentAppender\" > <appender ... > </appender> </appender> You can extract the nanotime using a converter: <!-- available as \"%nanoTime\" in a pattern layout --> <conversionRule conversionWord= \"nanoTime\" converterClass= \"com.tersesystems.logback.classic.NanoTimeConverter\" /> There are no configuration options.","title":"Relative Nanos"},{"location":"guide/relativens/#relative-nanoseconds-appender","text":"LoggingEvent already has a timestamp associated with it, but that timestamp is generated by System.currentTimeMillis . When your logging is fast enough that you can log several statements in the same millisecond, it can be frustrating to not know which came first. Adding a relative_ns field provides resolution down to the nanosecond, sort of . For example, here's two different records with the same millisecond. { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAA\" , \"relative_ns\" : 11808036 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"HikariPool-2 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } { \"id\" : \"FfwJtsNLLWo6O0Qbm7EAAB\" , \"relative_ns\" : 11981656 , \"tse_ms\" : 1584163603315 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:26:43.315Z\" , \"@version\" : \"1\" , \"message\" : \"jdbc-appender-pool-1584163602961 - Start completed.\" , \"logger_name\" : \"com.zaxxer.hikari.HikariDataSource\" , \"thread_name\" : \"logback-appender-ASYNC_JDBC-2\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } Note that the timestamp is 2020-03-14T05:26:43.315Z and the time since epoch is 1584163603315 . The flake ids distinguish between log entries by using a counter when millisecond precision is exceeded, so the first record is FfwJtsNLLWo6O0Qbm7EAAA ending in A and the second record is FfwJtsNLLWo6O0Qbm7EAAB ending in B . The relative time tells us exactly how much time has elapsed between the two events: 11981656 - 11808036 is 0.17362 milliseconds. All logging events are computed using System.nanoTime - NanoTime.start , where NanoTime.start is a static final field initialized JVM start time (technically at class loading but close enough). This value may be negative to begin with, but always increments. See the showcase for an example.","title":"Relative Nanoseconds Appender"},{"location":"guide/relativens/#usage","text":"<appender class= \"com.tersesystems.logback.classic.NanoTimeComponentAppender\" > <appender ... > </appender> </appender> You can extract the nanotime using a converter: <!-- available as \"%nanoTime\" in a pattern layout --> <conversionRule conversionWord= \"nanoTime\" converterClass= \"com.tersesystems.logback.classic.NanoTimeConverter\" /> There are no configuration options.","title":"Usage"},{"location":"guide/select/","text":"Select Appender \u00b6 Different appenders are useful in different environments. Development wants: Want colorized output on their consoles, with line oriented logs. Would also like to be able to read through logs with debug, info and warnings in them, to track control flow. If you have the logs seperated, that makes it harder. Generally don't want to run a local ELK stack or TCP appenders to see their logs. Operations wants: Really want centralized logging, and a way to drill out on it. Structured logging especially. May want to have everything write to STDOUT, as is case for Docker / 12 Factor Apps. May have duplicate logs from the underlying architecture, that need to be dedupped. May not want redundant / repeated messages, which developers are not as sensitive to. Really hate getting paged with the same error repeatedly. Logback is not aware of different environments. There's no out of the box way to say \"in this environment I want these sets of appenders, but in this other environment I want these other sets of appenders.\" Fortunately, adding this is pretty easy, by leveraging AppenderAttachable and pulling a key to select on: public class SelectAppender extends AppenderBase < ILoggingEvent > implements AppenderAttachable < ILoggingEvent > { private AppenderAttachableImpl < ILoggingEvent > aai = new AppenderAttachableImpl < ILoggingEvent > (); private String appenderKey ; @Override protected void append ( ILoggingEvent eventObject ) { Appender < ILoggingEvent > appender = aai . getAppender ( appenderKey ); if ( appender == null ) { addError ( \"No appender found for appenderKey \" + appenderKey ); } else { appender . doAppend ( eventObject ); } } // ... } The logback appenders under selection must have the name defined as an element, because Logback only looks for the name attribute at the top level, but otherwise they're the same. Here, we select the set of appenders we want based on the LOGBACK_ENVIRONMENT environment variable. <configuration> <appender name= \"SELECT\" class= \"com.tersesystems.logback.SelectAppender\" > <appenderKey> ${LOGBACK_ENVIRONMENT} </appenderKey> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> test </name> <appender class= \"ch.qos.logback.core.read.ListAppender\" > <name> test-list </name> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> development </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> development-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> staging </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> staging-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <appender class= \"ch.qos.logback.core.FileAppender\" > <name> staging-file </name> <file> file.log </file> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> </appender> <root level= \"TRACE\" > <appender-ref ref= \"SELECT\" /> </root> </configuration> This is a much cleaner way to organize appenders than putting Janino logic into the configuration.","title":"Select Appender"},{"location":"guide/select/#select-appender","text":"Different appenders are useful in different environments. Development wants: Want colorized output on their consoles, with line oriented logs. Would also like to be able to read through logs with debug, info and warnings in them, to track control flow. If you have the logs seperated, that makes it harder. Generally don't want to run a local ELK stack or TCP appenders to see their logs. Operations wants: Really want centralized logging, and a way to drill out on it. Structured logging especially. May want to have everything write to STDOUT, as is case for Docker / 12 Factor Apps. May have duplicate logs from the underlying architecture, that need to be dedupped. May not want redundant / repeated messages, which developers are not as sensitive to. Really hate getting paged with the same error repeatedly. Logback is not aware of different environments. There's no out of the box way to say \"in this environment I want these sets of appenders, but in this other environment I want these other sets of appenders.\" Fortunately, adding this is pretty easy, by leveraging AppenderAttachable and pulling a key to select on: public class SelectAppender extends AppenderBase < ILoggingEvent > implements AppenderAttachable < ILoggingEvent > { private AppenderAttachableImpl < ILoggingEvent > aai = new AppenderAttachableImpl < ILoggingEvent > (); private String appenderKey ; @Override protected void append ( ILoggingEvent eventObject ) { Appender < ILoggingEvent > appender = aai . getAppender ( appenderKey ); if ( appender == null ) { addError ( \"No appender found for appenderKey \" + appenderKey ); } else { appender . doAppend ( eventObject ); } } // ... } The logback appenders under selection must have the name defined as an element, because Logback only looks for the name attribute at the top level, but otherwise they're the same. Here, we select the set of appenders we want based on the LOGBACK_ENVIRONMENT environment variable. <configuration> <appender name= \"SELECT\" class= \"com.tersesystems.logback.SelectAppender\" > <appenderKey> ${LOGBACK_ENVIRONMENT} </appenderKey> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> test </name> <appender class= \"ch.qos.logback.core.read.ListAppender\" > <name> test-list </name> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> development </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> development-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> <appender class= \"com.tersesystems.logback.CompositeAppender\" > <name> staging </name> <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <name> staging-console </name> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> <appender class= \"ch.qos.logback.core.FileAppender\" > <name> staging-file </name> <file> file.log </file> <encoder> <pattern> %-5relative %-5level %logger{35} - %msg%n </pattern> </encoder> </appender> </appender> </appender> <root level= \"TRACE\" > <appender-ref ref= \"SELECT\" /> </root> </configuration> This is a much cleaner way to organize appenders than putting Janino logic into the configuration.","title":"Select Appender"},{"location":"guide/slf4jbridge/","text":"JUL to SLF4J Bridge \u00b6 It's easy to assume that all Java libraries will depend on SLF4J. But one of the oddities of Java logging is that there's a built-in logging framework called java.util.logging (JUL) which is rarely used but does appear in libraries such as Guice , GRPC , and Guava . When errors happen in these frameworks, they may never show up in logging at all, because JUL will write out to standard output and standard error by default. SLF4JBridgeHandler is a logging bridge, which is available in jul-to-slf4j . It does the job, but it does require some custom code to be added on startup to tell JUL that the handler is SLF4J: SLF4JBridgeHandler . removeHandlersForRootLogger (); SLF4JBridgeHandler . install (); This isn't ideal, as it's very easy to miss that you have to add these lines of code. Some frameworks such as Play Framework are smart enough are smart enough to handle this for you, but there are cases where you're not using those frameworks, and we'd like JUL to just work. This isn't so easy. JUL is very basic, and accepts configuration from system properties. The LogManager has two system properties: java.util.logging.config.class java.util.logging.config.file If it doesn't find either, then it looks in ${java.home}/conf/logging.properties if you're on JDK 11. There's no way to configure it from classpath, you have to do that by hand . There is discussion on Stack Overflow and the SLF4J mailing list suggesting that JUL looks for logging.properties in the classpath. This is incorrect -- the only way you'll see logging.properties is from setting java.util.logging.config.file or if you're overwriting ${java.home}/logging.properties . Here's the source code so you can check for yourself. However, since we're using Logback, we can leverage the fact that Logback searches through the classpath for logback.xml . All we need is a custom action to wrap SLF4JBridgeHandler and we can have a code free solution. This is what SLF4JBridgeHandlerAction does. You should also configure the LevelChangePropagator , to reduce the impact of logging , and you must make sure that the LoggerFactory is called before any JUL dependent code. You should set your logback.xml roughly as follows: <configuration> <!-- set up the rule --> <newRule pattern= \"configuration/slf4jBridgeHandler\" actionClass= \"com.tersesystems.logback.classic.SLF4JBridgeHandlerAction\" /> <!-- calls removeHandlersForRootLogger / install --> <slf4jBridgeHandler/> <!-- reset all previous level configurations of all j.u.l. loggers --> <contextListener class= \"ch.qos.logback.classic.jul.LevelChangePropagator\" > <resetJUL> true </resetJUL> </contextListener> <!-- Add Guice tracing --> <logger name= \"com.google.inject\" level= \"TRACE\" /> <root level= \"INFO\" > <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %date{H:mm:ss.SSS} [%highlight(%-5level)] %logger - %message%ex%n </pattern> </encoder> </appender> </root> </configuration> As of 0.17.0, logback-classic has a dependency on jul-to-slf4j so the following will work in build.gradle : dependencies { implementation \"com.google.inject:guice:5.0.1\" implementation 'com.tersesystems.logback:logback-classic:0.17.0' } And then you should call org.slf4j.LoggerFactory.getLogger as a static final to prevent any initialization problems: package example ; import com.google.inject.* ; import org.slf4j.* ; public class App { // Ensure that logback.xml is parsed by LoggerFactory _before_ Guice calls JUL. private static final Logger logger = org . slf4j . LoggerFactory . getLogger ( App . class ); public String getGreeting () { return \"Hello World!\" ; } public static void main ( String [] args ) { final Injector injector = Guice . createInjector (); final App instance = injector . getInstance ( App . class ); logger . info ( instance . getGreeting ()); } } And that should render the following: 19:51:45.493 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module execution: 64ms 19:51:45.494 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Interceptors creation: 2ms 19:51:45.496 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - TypeListeners & ProvisionListener creation: 1ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Scopes creation: 15ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Converters creation: 0ms 19:51:45.514 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding creation: 2ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module annotated method scanners creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Private environment creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Injector construction: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding initialization: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding indexing: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Collecting injection requests: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance member validation: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Provider verification: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Delayed Binding initialization: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static member injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Preloading singletons: 0ms 19:51:45.545 [INFO ] example.App - Hello World!","title":"JUL to SLF4J Bridge"},{"location":"guide/slf4jbridge/#jul-to-slf4j-bridge","text":"It's easy to assume that all Java libraries will depend on SLF4J. But one of the oddities of Java logging is that there's a built-in logging framework called java.util.logging (JUL) which is rarely used but does appear in libraries such as Guice , GRPC , and Guava . When errors happen in these frameworks, they may never show up in logging at all, because JUL will write out to standard output and standard error by default. SLF4JBridgeHandler is a logging bridge, which is available in jul-to-slf4j . It does the job, but it does require some custom code to be added on startup to tell JUL that the handler is SLF4J: SLF4JBridgeHandler . removeHandlersForRootLogger (); SLF4JBridgeHandler . install (); This isn't ideal, as it's very easy to miss that you have to add these lines of code. Some frameworks such as Play Framework are smart enough are smart enough to handle this for you, but there are cases where you're not using those frameworks, and we'd like JUL to just work. This isn't so easy. JUL is very basic, and accepts configuration from system properties. The LogManager has two system properties: java.util.logging.config.class java.util.logging.config.file If it doesn't find either, then it looks in ${java.home}/conf/logging.properties if you're on JDK 11. There's no way to configure it from classpath, you have to do that by hand . There is discussion on Stack Overflow and the SLF4J mailing list suggesting that JUL looks for logging.properties in the classpath. This is incorrect -- the only way you'll see logging.properties is from setting java.util.logging.config.file or if you're overwriting ${java.home}/logging.properties . Here's the source code so you can check for yourself. However, since we're using Logback, we can leverage the fact that Logback searches through the classpath for logback.xml . All we need is a custom action to wrap SLF4JBridgeHandler and we can have a code free solution. This is what SLF4JBridgeHandlerAction does. You should also configure the LevelChangePropagator , to reduce the impact of logging , and you must make sure that the LoggerFactory is called before any JUL dependent code. You should set your logback.xml roughly as follows: <configuration> <!-- set up the rule --> <newRule pattern= \"configuration/slf4jBridgeHandler\" actionClass= \"com.tersesystems.logback.classic.SLF4JBridgeHandlerAction\" /> <!-- calls removeHandlersForRootLogger / install --> <slf4jBridgeHandler/> <!-- reset all previous level configurations of all j.u.l. loggers --> <contextListener class= \"ch.qos.logback.classic.jul.LevelChangePropagator\" > <resetJUL> true </resetJUL> </contextListener> <!-- Add Guice tracing --> <logger name= \"com.google.inject\" level= \"TRACE\" /> <root level= \"INFO\" > <appender class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder> <pattern> %date{H:mm:ss.SSS} [%highlight(%-5level)] %logger - %message%ex%n </pattern> </encoder> </appender> </root> </configuration> As of 0.17.0, logback-classic has a dependency on jul-to-slf4j so the following will work in build.gradle : dependencies { implementation \"com.google.inject:guice:5.0.1\" implementation 'com.tersesystems.logback:logback-classic:0.17.0' } And then you should call org.slf4j.LoggerFactory.getLogger as a static final to prevent any initialization problems: package example ; import com.google.inject.* ; import org.slf4j.* ; public class App { // Ensure that logback.xml is parsed by LoggerFactory _before_ Guice calls JUL. private static final Logger logger = org . slf4j . LoggerFactory . getLogger ( App . class ); public String getGreeting () { return \"Hello World!\" ; } public static void main ( String [] args ) { final Injector injector = Guice . createInjector (); final App instance = injector . getInstance ( App . class ); logger . info ( instance . getGreeting ()); } } And that should render the following: 19:51:45.493 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module execution: 64ms 19:51:45.494 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Interceptors creation: 2ms 19:51:45.496 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - TypeListeners & ProvisionListener creation: 1ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Scopes creation: 15ms 19:51:45.511 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Converters creation: 0ms 19:51:45.514 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding creation: 2ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Module annotated method scanners creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Private environment creation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Injector construction: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding initialization: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding indexing: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Collecting injection requests: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Binding validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static validation: 0ms 19:51:45.515 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance member validation: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Provider verification: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Delayed Binding initialization: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Static member injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Instance injection: 0ms 19:51:45.516 [DEBUG] com.google.inject.internal.util.ContinuousStopwatch - Preloading singletons: 0ms 19:51:45.545 [INFO ] example.App - Hello World!","title":"JUL to SLF4J Bridge"},{"location":"guide/tracing/","text":"Tracing to Honeycomb \u00b6 You can connect Logback to Honeycomb directly through the Honeycomb Logback appender. The appender is split into the appender and an HTTP client implementation, which can be OKHTTP or Play WS. Add the appender module 'logback-honeycomb-appender' and the implementation 'logback-honeycomb-okhttp': implementation group: 'com.tersesystems.logback', name: 'logback-tracing' implementation group: 'com.tersesystems.logback', name: 'logback-honeycomb-okhttp' The appender is of type com.tersesystems.logback.honeycomb.HoneycombAppender , and makes use of the client under the hood. Because the honeycomb appender uses an HTTP client under the hood, there are a couple of important notes. NOTE : Because the HTTP client runs on a different thread, you must have a shutdown hook configured so that shutting down can be delayed until the events are posted. and NOTE : Because the HTTP client may have non-daemon threads running, you should call System.exit explicitly to stop the application if you are not in a long-running service. The appender is as follows: <configuration> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" > <delay> 1000 </delay> </shutdownHook> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <appender name= \"HONEYCOMB\" class= \"com.tersesystems.logback.honeycomb.HoneycombAppender\" > <apiKey> ${HONEYCOMB_API_KEY} </apiKey> <dataSet> terse-logback </dataSet> <sampleRate> 1 </sampleRate> <queueSize> 10 </queueSize> <batch> true </batch> <includeCallerData> false </includeCallerData> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> <!-- don't send the logs from the http engine to the appender or you may end up in a loop --> <logger name= \"okhttp\" level= \"ERROR\" /> <root level= \"INFO\" > <appender-ref ref= \"HONEYCOMB\" /> </root> </configuration> You can also send tracing information to Honeycomb through SLF4J markers, using the SpanMarkerFactory . Underneath the hood, the SpanInfo puts together logstash markers according to manual tracing . The way this works in practice is that you start up a SpanInfo at the beginning of a request, and call buildNow to mark the start of the span. At the end of the operation, you log with a marker, by passing through the marker factory: SpanInfo spanInfo = builder . setRootSpan ( \"index\" ). buildNow (); // ... logger . info ( markerFactory . apply ( spanInfo ), \"completed successfully!\" ); If you want to create a child span, you can do it from the parent using withChild : return spanInfo . withChild ( \"doSomething\" , childInfo -> { return doSomething ( childInfo ); }); or asking for a child builder that you can build yourself: SpanInfo childInfo = spanInfo . childBuilder (). setSpanName ( \"doSomething\" ). buildNow (); The start time information is captured in a StartTimeMarker which can be extracted by StartTime.from and is used in building the Honeycomb Request. The event timestamp serves as the span's end time. This is useful in Honeycomb Tracing, as the timestamp is the start time, not the time that the log entry was posted. For example, in Play you might run a controller as follows: import com . tersesystems . logback . tracing . SpanMarkerFactory import com . tersesystems . logback . tracing . SpanInfo import javax . inject . _ import org . slf4j . LoggerFactory import play . api . libs . concurrent . Futures import play . api . mvc . _ import scala . concurrent .{ ExecutionContext , Future } import scala . util .{ Failure , Success } import scala . concurrent . duration . _ @Singleton class HomeController @Inject ()( cc : ControllerComponents , futures : Futures ) ( implicit ec : ExecutionContext ) extends AbstractController ( cc ) { private val markerFactory = new SpanMarkerFactory () private val logger = LoggerFactory . getLogger ( getClass ) private def builder : SpanInfo . Builder = SpanInfo . builder (). setServiceName ( \"play_hello_world\" ) def index (): Action [ AnyContent ] = Action . async { implicit request : Request [ AnyContent ] => val spanInfo = builder . setRootSpan ( \"index\" ). buildNow () val f : Future [ Result ] = spanInfo . withChild ( \"renderPage\" , renderPage ( _ )) f . andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"index completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"index completed with error\" , e ) } } def renderPage ( spanInfo : SpanInfo ): Future [ Result ] = { futures . delay ( 5 . seconds ). map { _ => Ok ( views . html . index ()) }. andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"renderPage completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"renderPage completed with error\" , e ) } } } This generates a trace with a root span of \"index\", a child span of \"renderPage\" each with their own durations. You can also send span events and span links using the LinkMarkerFactory and EventMarkerFactory , similar to the SpanMarkerFactory . See Tracing With Logback and Honeycomb and Hierarchical Instrumented Tracing with Logback for more details.","title":"Tracing with Honeycomb"},{"location":"guide/tracing/#tracing-to-honeycomb","text":"You can connect Logback to Honeycomb directly through the Honeycomb Logback appender. The appender is split into the appender and an HTTP client implementation, which can be OKHTTP or Play WS. Add the appender module 'logback-honeycomb-appender' and the implementation 'logback-honeycomb-okhttp': implementation group: 'com.tersesystems.logback', name: 'logback-tracing' implementation group: 'com.tersesystems.logback', name: 'logback-honeycomb-okhttp' The appender is of type com.tersesystems.logback.honeycomb.HoneycombAppender , and makes use of the client under the hood. Because the honeycomb appender uses an HTTP client under the hood, there are a couple of important notes. NOTE : Because the HTTP client runs on a different thread, you must have a shutdown hook configured so that shutting down can be delayed until the events are posted. and NOTE : Because the HTTP client may have non-daemon threads running, you should call System.exit explicitly to stop the application if you are not in a long-running service. The appender is as follows: <configuration> <shutdownHook class= \"ch.qos.logback.core.hook.DelayingShutdownHook\" > <delay> 1000 </delay> </shutdownHook> <conversionRule conversionWord= \"startTime\" converterClass= \"com.tersesystems.logback.classic.StartTimeConverter\" /> <appender name= \"HONEYCOMB\" class= \"com.tersesystems.logback.honeycomb.HoneycombAppender\" > <apiKey> ${HONEYCOMB_API_KEY} </apiKey> <dataSet> terse-logback </dataSet> <sampleRate> 1 </sampleRate> <queueSize> 10 </queueSize> <batch> true </batch> <includeCallerData> false </includeCallerData> <encoder class= \"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\" > <providers> <message/> <loggerName/> <threadName/> <logLevel/> <stackHash/> <mdc/> <logstashMarkers/> <pattern> <pattern> { \"start_ms\": \"#asLong{%startTime}\" } </pattern> </pattern> <arguments/> <stackTrace> <throwableConverter class= \"net.logstash.logback.stacktrace.ShortenedThrowableConverter\" > <rootCauseFirst> true </rootCauseFirst> </throwableConverter> </stackTrace> </providers> </encoder> </appender> <!-- don't send the logs from the http engine to the appender or you may end up in a loop --> <logger name= \"okhttp\" level= \"ERROR\" /> <root level= \"INFO\" > <appender-ref ref= \"HONEYCOMB\" /> </root> </configuration> You can also send tracing information to Honeycomb through SLF4J markers, using the SpanMarkerFactory . Underneath the hood, the SpanInfo puts together logstash markers according to manual tracing . The way this works in practice is that you start up a SpanInfo at the beginning of a request, and call buildNow to mark the start of the span. At the end of the operation, you log with a marker, by passing through the marker factory: SpanInfo spanInfo = builder . setRootSpan ( \"index\" ). buildNow (); // ... logger . info ( markerFactory . apply ( spanInfo ), \"completed successfully!\" ); If you want to create a child span, you can do it from the parent using withChild : return spanInfo . withChild ( \"doSomething\" , childInfo -> { return doSomething ( childInfo ); }); or asking for a child builder that you can build yourself: SpanInfo childInfo = spanInfo . childBuilder (). setSpanName ( \"doSomething\" ). buildNow (); The start time information is captured in a StartTimeMarker which can be extracted by StartTime.from and is used in building the Honeycomb Request. The event timestamp serves as the span's end time. This is useful in Honeycomb Tracing, as the timestamp is the start time, not the time that the log entry was posted. For example, in Play you might run a controller as follows: import com . tersesystems . logback . tracing . SpanMarkerFactory import com . tersesystems . logback . tracing . SpanInfo import javax . inject . _ import org . slf4j . LoggerFactory import play . api . libs . concurrent . Futures import play . api . mvc . _ import scala . concurrent .{ ExecutionContext , Future } import scala . util .{ Failure , Success } import scala . concurrent . duration . _ @Singleton class HomeController @Inject ()( cc : ControllerComponents , futures : Futures ) ( implicit ec : ExecutionContext ) extends AbstractController ( cc ) { private val markerFactory = new SpanMarkerFactory () private val logger = LoggerFactory . getLogger ( getClass ) private def builder : SpanInfo . Builder = SpanInfo . builder (). setServiceName ( \"play_hello_world\" ) def index (): Action [ AnyContent ] = Action . async { implicit request : Request [ AnyContent ] => val spanInfo = builder . setRootSpan ( \"index\" ). buildNow () val f : Future [ Result ] = spanInfo . withChild ( \"renderPage\" , renderPage ( _ )) f . andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"index completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"index completed with error\" , e ) } } def renderPage ( spanInfo : SpanInfo ): Future [ Result ] = { futures . delay ( 5 . seconds ). map { _ => Ok ( views . html . index ()) }. andThen { case Success ( _ ) => logger . info ( markerFactory ( spanInfo ), \"renderPage completed successfully!\" ) case Failure ( e ) => logger . error ( markerFactory ( spanInfo ), \"renderPage completed with error\" , e ) } } } This generates a trace with a root span of \"index\", a child span of \"renderPage\" each with their own durations. You can also send span events and span links using the LinkMarkerFactory and EventMarkerFactory , similar to the SpanMarkerFactory . See Tracing With Logback and Honeycomb and Hierarchical Instrumented Tracing with Logback for more details.","title":"Tracing to Honeycomb"},{"location":"guide/turbomarker/","text":"Turbo Markers \u00b6 Turbo filters are filters that decide whether a logging event should be created or not. They are not appender specific in the way that normal filters are, and so are used to override logger levels. However, there's a problem with the way that the turbo filter is set up: the two implementing classes are ch.qos.logback.classic.turbo.MarkerFilter and ch.qos.logback.classic.turbo.MDCFilter . The marker filter will always log if the given marker is applied, and the MDC filter relies on an attribute being populated in the MDC map. What we'd really like to do is say \"for this particular user, log everything he does at DEBUG level\" and not have it rely on thread-local state at all, and carry out an arbitrary computation at call time. We start by pulling the decide method to an interface, TurboFilterDecider : public interface TurboFilterDecider { FilterReply decide ( Marker marker , Logger logger , Level level , String format , Object [] params , Throwable t ); } And have the turbo filter delegate to markers that implement the TurboFilterDecider interface : public class TurboMarkerTurboFilter extends TurboFilter { @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { // ... } private FilterReply evaluateMarker ( Marker marker , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { if ( marker instanceof TurboFilterDecider ) { TurboFilterDecider decider = ( TurboFilterDecider ) marker ; return decider . decide ( rootMarker , logger , level , format , params , t ); } return FilterReply . NEUTRAL ; } } This gets us part of the way there. We can then set up a ContextAwareTurboFilterDecider , which does the same thing but assumes that you have a type C that is your external context. public interface ContextAwareTurboFilterDecider < C > { FilterReply decide ( ContextAwareTurboMarker < C > marker , C context , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ); } Then we add a marker class that incorporates that context in decision making : public class ContextAwareTurboMarker < C > extends TurboMarker implements TurboFilterDecider { private final C context ; private final ContextAwareTurboFilterDecider < C > contextAwareDecider ; // ... initializers and such @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { return contextAwareDecider . decide ( this , context , rootMarker , logger , level , format , params , t ); } } This may look good in the abstract, but it may make more sense to see it in action. To do this, we'll set up an example application context: public class ApplicationContext { private final String userId ; public ApplicationContext ( String userId ) { this . userId = userId ; } public String currentUserId () { return userId ; } } and a factory that contains the decider: import com.tersesystems.logback.turbomarker.* ; public class UserMarkerFactory { private final Set < String > userIdSet = new ConcurrentSkipListSet <> (); private final ContextDecider < ApplicationContext > decider = context -> userIdSet . contains ( context . currentUserId ()) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; public void addUserId ( String userId ) { userIdSet . add ( userId ); } public void clear () { userIdSet . clear (); } public UserMarker create ( ApplicationContext applicationContext ) { return new UserMarker ( \"userMarker\" , applicationContext , decider ); } } and a UserMarker , which is only around for the logging evaluation: public class UserMarker extends ContextAwareTurboMarker < ApplicationContext > { public UserMarker ( String name , ApplicationContext applicationContext , ContextAwareTurboFilterDecider < ApplicationContext > decider ) { super ( name , applicationContext , decider ); } } and then we can set up logging that will only work for user \"28\": String userId = \"28\" ; ApplicationContext applicationContext = new ApplicationContext ( userId ); UserMarkerFactory userMarkerFactory = new UserMarkerFactory (); userMarkerFactory . addUserId ( userId ); // say we want logging events created for this user id UserMarker userMarker = userMarkerFactory . create ( applicationContext ); logger . info ( userMarker , \"Hello world, I am info and log for everyone\" ); logger . debug ( userMarker , \"Hello world, I am debug and only log for user 28\" ); This works especially well with a configuration management service like Launch Darkly , where you can target particular users and set up logging based on the user variation. The LaunchDarkly blog has best practices for operational flags : Verbose logs are great for debugging and troubleshooting but always running an application in debug mode is not viable. The amount of log data generated would be overwhelming. Changing logging levels on the fly typically requires changing a configuration file and restarting the application. A multivariate operational flag enables you to change the logging level from WARNING to DEBUG in real-time. But we can give an example using the Java SDK. You can set up a factory like so: import ch.qos.logback.classic.Logger ; import ch.qos.logback.core.spi.FilterReply ; import com.launchdarkly.client.LDClientInterface ; import com.launchdarkly.client.LDUser ; public class LDMarkerFactory { private final LaunchDarklyDecider decider ; public LDMarkerFactory ( LDClientInterface client ) { this . decider = new LaunchDarklyDecider ( requireNonNull ( client )); } public LDMarker create ( String featureFlag , LDUser user ) { return new LDMarker ( featureFlag , user , decider ); } static class LaunchDarklyDecider implements MarkerContextDecider < LDUser > { private final LDClientInterface ldClient ; LaunchDarklyDecider ( LDClientInterface ldClient ) { this . ldClient = ldClient ; } @Override public FilterReply apply ( ContextAwareTurboMarker < LDUser > marker , LDUser ldUser ) { return ldClient . boolVariation ( marker . getName (), ldUser , false ) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; } } public static class LDMarker extends ContextAwareTurboMarker < LDUser > { LDMarker ( String name , LDUser context , ContextAwareTurboFilterDecider < LDUser > decider ) { super ( name , context , decider ); } } } and then use the feature flag as the marker name and target the beta testers group: public class LDMarkerTest { private static LDClientInterface client ; @BeforeAll public static void setUp () { client = new LDClient ( \"sdk-key\" ); } @AfterAll public static void shutDown () { try { client . close (); } catch ( IOException e ) { e . printStackTrace (); } } public void testMatchingMarker () throws JoranException { LoggerContext loggerContext = ( LoggerContext ) LoggerFactory . getILoggerFactory (); ch . qos . logback . classic . Logger logger = loggerContext . getLogger ( org . slf4j . Logger . ROOT_LOGGER_NAME ); LDMarkerFactory markerFactory = new LDMarkerFactory ( client ); LDUser ldUser = new LDUser . Builder ( \"UNIQUE IDENTIFIER\" ) . firstName ( \"Bob\" ) . lastName ( \"Loblaw\" ) . customString ( \"groups\" , singletonList ( \"beta_testers\" )) . build (); LDMarkerFactory . LDMarker ldMarker = markerFactory . create ( \"turbomarker\" , ldUser ); logger . info ( ldMarker , \"Hello world, I am info\" ); logger . debug ( ldMarker , \"Hello world, I am debug\" ); ListAppender < ILoggingEvent > appender = ( ListAppender < ILoggingEvent > ) logger . getAppender ( \"LIST\" ); assertThat ( appender . list . size ()). isEqualTo ( 2 ); appender . list . clear (); } } This is also a reason why diagnostic logging is better than a debugger . Debuggers are ephemeral, can't be used in production, and don't produce a consistent record of events: debugging log statements are the single best way to dump internal state and manage code flows in an application. See Targeted Diagnostic Logging in Production for more details.","title":"Turbo Markers"},{"location":"guide/turbomarker/#turbo-markers","text":"Turbo filters are filters that decide whether a logging event should be created or not. They are not appender specific in the way that normal filters are, and so are used to override logger levels. However, there's a problem with the way that the turbo filter is set up: the two implementing classes are ch.qos.logback.classic.turbo.MarkerFilter and ch.qos.logback.classic.turbo.MDCFilter . The marker filter will always log if the given marker is applied, and the MDC filter relies on an attribute being populated in the MDC map. What we'd really like to do is say \"for this particular user, log everything he does at DEBUG level\" and not have it rely on thread-local state at all, and carry out an arbitrary computation at call time. We start by pulling the decide method to an interface, TurboFilterDecider : public interface TurboFilterDecider { FilterReply decide ( Marker marker , Logger logger , Level level , String format , Object [] params , Throwable t ); } And have the turbo filter delegate to markers that implement the TurboFilterDecider interface : public class TurboMarkerTurboFilter extends TurboFilter { @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { // ... } private FilterReply evaluateMarker ( Marker marker , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { if ( marker instanceof TurboFilterDecider ) { TurboFilterDecider decider = ( TurboFilterDecider ) marker ; return decider . decide ( rootMarker , logger , level , format , params , t ); } return FilterReply . NEUTRAL ; } } This gets us part of the way there. We can then set up a ContextAwareTurboFilterDecider , which does the same thing but assumes that you have a type C that is your external context. public interface ContextAwareTurboFilterDecider < C > { FilterReply decide ( ContextAwareTurboMarker < C > marker , C context , Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ); } Then we add a marker class that incorporates that context in decision making : public class ContextAwareTurboMarker < C > extends TurboMarker implements TurboFilterDecider { private final C context ; private final ContextAwareTurboFilterDecider < C > contextAwareDecider ; // ... initializers and such @Override public FilterReply decide ( Marker rootMarker , Logger logger , Level level , String format , Object [] params , Throwable t ) { return contextAwareDecider . decide ( this , context , rootMarker , logger , level , format , params , t ); } } This may look good in the abstract, but it may make more sense to see it in action. To do this, we'll set up an example application context: public class ApplicationContext { private final String userId ; public ApplicationContext ( String userId ) { this . userId = userId ; } public String currentUserId () { return userId ; } } and a factory that contains the decider: import com.tersesystems.logback.turbomarker.* ; public class UserMarkerFactory { private final Set < String > userIdSet = new ConcurrentSkipListSet <> (); private final ContextDecider < ApplicationContext > decider = context -> userIdSet . contains ( context . currentUserId ()) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; public void addUserId ( String userId ) { userIdSet . add ( userId ); } public void clear () { userIdSet . clear (); } public UserMarker create ( ApplicationContext applicationContext ) { return new UserMarker ( \"userMarker\" , applicationContext , decider ); } } and a UserMarker , which is only around for the logging evaluation: public class UserMarker extends ContextAwareTurboMarker < ApplicationContext > { public UserMarker ( String name , ApplicationContext applicationContext , ContextAwareTurboFilterDecider < ApplicationContext > decider ) { super ( name , applicationContext , decider ); } } and then we can set up logging that will only work for user \"28\": String userId = \"28\" ; ApplicationContext applicationContext = new ApplicationContext ( userId ); UserMarkerFactory userMarkerFactory = new UserMarkerFactory (); userMarkerFactory . addUserId ( userId ); // say we want logging events created for this user id UserMarker userMarker = userMarkerFactory . create ( applicationContext ); logger . info ( userMarker , \"Hello world, I am info and log for everyone\" ); logger . debug ( userMarker , \"Hello world, I am debug and only log for user 28\" ); This works especially well with a configuration management service like Launch Darkly , where you can target particular users and set up logging based on the user variation. The LaunchDarkly blog has best practices for operational flags : Verbose logs are great for debugging and troubleshooting but always running an application in debug mode is not viable. The amount of log data generated would be overwhelming. Changing logging levels on the fly typically requires changing a configuration file and restarting the application. A multivariate operational flag enables you to change the logging level from WARNING to DEBUG in real-time. But we can give an example using the Java SDK. You can set up a factory like so: import ch.qos.logback.classic.Logger ; import ch.qos.logback.core.spi.FilterReply ; import com.launchdarkly.client.LDClientInterface ; import com.launchdarkly.client.LDUser ; public class LDMarkerFactory { private final LaunchDarklyDecider decider ; public LDMarkerFactory ( LDClientInterface client ) { this . decider = new LaunchDarklyDecider ( requireNonNull ( client )); } public LDMarker create ( String featureFlag , LDUser user ) { return new LDMarker ( featureFlag , user , decider ); } static class LaunchDarklyDecider implements MarkerContextDecider < LDUser > { private final LDClientInterface ldClient ; LaunchDarklyDecider ( LDClientInterface ldClient ) { this . ldClient = ldClient ; } @Override public FilterReply apply ( ContextAwareTurboMarker < LDUser > marker , LDUser ldUser ) { return ldClient . boolVariation ( marker . getName (), ldUser , false ) ? FilterReply . ACCEPT : FilterReply . NEUTRAL ; } } public static class LDMarker extends ContextAwareTurboMarker < LDUser > { LDMarker ( String name , LDUser context , ContextAwareTurboFilterDecider < LDUser > decider ) { super ( name , context , decider ); } } } and then use the feature flag as the marker name and target the beta testers group: public class LDMarkerTest { private static LDClientInterface client ; @BeforeAll public static void setUp () { client = new LDClient ( \"sdk-key\" ); } @AfterAll public static void shutDown () { try { client . close (); } catch ( IOException e ) { e . printStackTrace (); } } public void testMatchingMarker () throws JoranException { LoggerContext loggerContext = ( LoggerContext ) LoggerFactory . getILoggerFactory (); ch . qos . logback . classic . Logger logger = loggerContext . getLogger ( org . slf4j . Logger . ROOT_LOGGER_NAME ); LDMarkerFactory markerFactory = new LDMarkerFactory ( client ); LDUser ldUser = new LDUser . Builder ( \"UNIQUE IDENTIFIER\" ) . firstName ( \"Bob\" ) . lastName ( \"Loblaw\" ) . customString ( \"groups\" , singletonList ( \"beta_testers\" )) . build (); LDMarkerFactory . LDMarker ldMarker = markerFactory . create ( \"turbomarker\" , ldUser ); logger . info ( ldMarker , \"Hello world, I am info\" ); logger . debug ( ldMarker , \"Hello world, I am debug\" ); ListAppender < ILoggingEvent > appender = ( ListAppender < ILoggingEvent > ) logger . getAppender ( \"LIST\" ); assertThat ( appender . list . size ()). isEqualTo ( 2 ); appender . list . clear (); } } This is also a reason why diagnostic logging is better than a debugger . Debuggers are ephemeral, can't be used in production, and don't produce a consistent record of events: debugging log statements are the single best way to dump internal state and manage code flows in an application. See Targeted Diagnostic Logging in Production for more details.","title":"Turbo Markers"},{"location":"guide/typesafeconfig/","text":"Typesafe Config \u00b6 The TypesafeConfigAction will search in a variety of places for configuration using standard fallback behavior for Typesafe Config, which gives a richer experience to end users. Config config = systemProperties // Look for a property from system properties first... . withFallback ( file ) // if we don't find it, then look in an explicitly defined file... . withFallback ( testResources ) // if not, then if logback-test.conf exists, look for it there... . withFallback ( resources ) // then look in logback.conf... . withFallback ( reference ) // and then finally in logback-reference.conf. . resolve (); // Tell config that we want to use ${?ENV_VAR} type stuff. The configuration is then placed in the LoggerContext which is available to all of Logback. lc . putObject ( ConfigConstants . TYPESAFE_CONFIG_CTX_KEY , config ); And then all properties are made available to Logback, either at the local scope or at the context scope. Properties must be strings, but you can also provide Maps and Lists to the Logback Context, through context.getObject . Log Levels and Properties through Typesafe Config \u00b6 Configuration of properties and setting log levels is done through Typesafe Config , using TypesafeConfigAction Here's the logback.conf from the example application. It's in Human-Optimized Config Object Notation or HOCON . # Set logger levels here. levels = { # Override the default root log level with ROOT_LOG_LEVEL environment variable, if defined... ROOT = ${?ROOT_LOG_LEVEL} # You can set a logger with a simple package name. example = DEBUG # You can also do nested overrides here. deeply.nested { package = TRACE } } # Overrides the properties from logback-reference.conf local { logback.environment=production censor { regex = \"\"\"hunter2\"\"\" // http://bash.org/?244321 replacementText = \"*******\" json.keys += \"password\" // adding password key will remove the key/value pair entirely } # Overwrite text file on every run. textfile { append = false } # Override the color code in console for info statements highlight { info = \"black\" } } # You can also include settings from other places include \"myothersettings\" For tests, there's a logback-test.conf that will override (rather than completely replace) any settings that you have in logback.conf : include \"logback-reference\" levels { example = TRACE } local { logback.environment=test textfile { location = \"log/test/application-test.log\" append = false } jsonfile { location = \"log/test/application-test.json\" prettyprint = true } } There is also a logback-reference.conf file that handles the default configuration for the appenders, and those settings can be overridden. They are written out individually in the encoder configuration so I won't go over it here. Note that appender logic is not available here -- it's all defined through the structured-config in logback.xml . Using Typesafe Config is not a requirement -- the point here is to show that there are more options to configuring Logback than using a straight XML file. See Application Logging in Java: Adding Configuration for more details.","title":"Typesafe Config"},{"location":"guide/typesafeconfig/#typesafe-config","text":"The TypesafeConfigAction will search in a variety of places for configuration using standard fallback behavior for Typesafe Config, which gives a richer experience to end users. Config config = systemProperties // Look for a property from system properties first... . withFallback ( file ) // if we don't find it, then look in an explicitly defined file... . withFallback ( testResources ) // if not, then if logback-test.conf exists, look for it there... . withFallback ( resources ) // then look in logback.conf... . withFallback ( reference ) // and then finally in logback-reference.conf. . resolve (); // Tell config that we want to use ${?ENV_VAR} type stuff. The configuration is then placed in the LoggerContext which is available to all of Logback. lc . putObject ( ConfigConstants . TYPESAFE_CONFIG_CTX_KEY , config ); And then all properties are made available to Logback, either at the local scope or at the context scope. Properties must be strings, but you can also provide Maps and Lists to the Logback Context, through context.getObject .","title":"Typesafe Config"},{"location":"guide/typesafeconfig/#log-levels-and-properties-through-typesafe-config","text":"Configuration of properties and setting log levels is done through Typesafe Config , using TypesafeConfigAction Here's the logback.conf from the example application. It's in Human-Optimized Config Object Notation or HOCON . # Set logger levels here. levels = { # Override the default root log level with ROOT_LOG_LEVEL environment variable, if defined... ROOT = ${?ROOT_LOG_LEVEL} # You can set a logger with a simple package name. example = DEBUG # You can also do nested overrides here. deeply.nested { package = TRACE } } # Overrides the properties from logback-reference.conf local { logback.environment=production censor { regex = \"\"\"hunter2\"\"\" // http://bash.org/?244321 replacementText = \"*******\" json.keys += \"password\" // adding password key will remove the key/value pair entirely } # Overwrite text file on every run. textfile { append = false } # Override the color code in console for info statements highlight { info = \"black\" } } # You can also include settings from other places include \"myothersettings\" For tests, there's a logback-test.conf that will override (rather than completely replace) any settings that you have in logback.conf : include \"logback-reference\" levels { example = TRACE } local { logback.environment=test textfile { location = \"log/test/application-test.log\" append = false } jsonfile { location = \"log/test/application-test.json\" prettyprint = true } } There is also a logback-reference.conf file that handles the default configuration for the appenders, and those settings can be overridden. They are written out individually in the encoder configuration so I won't go over it here. Note that appender logic is not available here -- it's all defined through the structured-config in logback.xml . Using Typesafe Config is not a requirement -- the point here is to show that there are more options to configuring Logback than using a straight XML file. See Application Logging in Java: Adding Configuration for more details.","title":"Log Levels and Properties through Typesafe Config"},{"location":"guide/uniqueid/","text":"Unique ID Appenders \u00b6 The unique id appender allows the logging event to carry a unique id. When used in conjunction with SelectAppender or CompositeAppender , this allows for a log record to use the same id across different logs. For example, in application.log , you'll see a single line that starts with FfwJtsNHYSw6O0Qbm7EAAA : FfwJtsNHYSw6O0Qbm7EAAA 2020-03-14T05:30:14.965+0000 [INFO ] play.api.db.HikariCPConnectionPool in play-dev-mode-akka.actor.default-dispatcher-7 - Creating Pool for datasource 'logging' You can search for this string in application.json and see more detail on the log record: { \"id\" : \"FfwJtsNHYSw6O0Qbm7EAAA\" , \"relative_ns\" : 20921024 , \"tse_ms\" : 1584163814965 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:30:14.965Z\" , \"@version\" : \"1\" , \"message\" : \"Creating Pool for datasource 'logging'\" , \"logger_name\" : \"play.api.db.HikariCPConnectionPool\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } See the showcase for an example. Usage \u00b6 <appender name= \"selector-with-unique-id\" class= \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > <appender ... > </appender> </appender> To extract the unique ID, register a converter: <!-- available as \"%uniqueId\" in a pattern layout --> <conversionRule conversionWord= \"uniqueId\" converterClass= \"com.tersesystems.logback.uniqueid.UniqueIdConverter\" /> ID Generators \u00b6 Unique IDs come with two different options. Flake ID is the default. Random UUID \u00b6 This implementation uses RandomBasedGenerator from Java UUID Generator . This is faster than using java.util.UUID.randomUUID , because it avoids the synchronization lock . Flake ID \u00b6 Flake IDs are decentralized and k-ordered, meaning that they are \"roughly time-ordered when sorted lexicographically.\" This implementation uses idem with Flake128S .","title":"Unique ID Appender"},{"location":"guide/uniqueid/#unique-id-appenders","text":"The unique id appender allows the logging event to carry a unique id. When used in conjunction with SelectAppender or CompositeAppender , this allows for a log record to use the same id across different logs. For example, in application.log , you'll see a single line that starts with FfwJtsNHYSw6O0Qbm7EAAA : FfwJtsNHYSw6O0Qbm7EAAA 2020-03-14T05:30:14.965+0000 [INFO ] play.api.db.HikariCPConnectionPool in play-dev-mode-akka.actor.default-dispatcher-7 - Creating Pool for datasource 'logging' You can search for this string in application.json and see more detail on the log record: { \"id\" : \"FfwJtsNHYSw6O0Qbm7EAAA\" , \"relative_ns\" : 20921024 , \"tse_ms\" : 1584163814965 , \"start_ms\" : null , \"@timestamp\" : \"2020-03-14T05:30:14.965Z\" , \"@version\" : \"1\" , \"message\" : \"Creating Pool for datasource 'logging'\" , \"logger_name\" : \"play.api.db.HikariCPConnectionPool\" , \"thread_name\" : \"play-dev-mode-akka.actor.default-dispatcher-7\" , \"level\" : \"INFO\" , \"level_value\" : 20000 } See the showcase for an example.","title":"Unique ID Appenders"},{"location":"guide/uniqueid/#usage","text":"<appender name= \"selector-with-unique-id\" class= \"com.tersesystems.logback.uniqueid.UniqueIdComponentAppender\" > <appender ... > </appender> </appender> To extract the unique ID, register a converter: <!-- available as \"%uniqueId\" in a pattern layout --> <conversionRule conversionWord= \"uniqueId\" converterClass= \"com.tersesystems.logback.uniqueid.UniqueIdConverter\" />","title":"Usage"},{"location":"guide/uniqueid/#id-generators","text":"Unique IDs come with two different options. Flake ID is the default.","title":"ID Generators"},{"location":"guide/uniqueid/#random-uuid","text":"This implementation uses RandomBasedGenerator from Java UUID Generator . This is faster than using java.util.UUID.randomUUID , because it avoids the synchronization lock .","title":"Random UUID"},{"location":"guide/uniqueid/#flake-id","text":"Flake IDs are decentralized and k-ordered, meaning that they are \"roughly time-ordered when sorted lexicographically.\" This implementation uses idem with Flake128S .","title":"Flake ID"}]}